% Template for PLoS
% Version 1.0 January 2009
%
% To compile to pdf, run:
% latex plos.template
% bibtex plos.template
% latex plos.template
% latex plos.template
% dvipdf plos.template

\documentclass[10pt]{article}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}
% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

\usepackage{color} 

% Use doublespacing - comment out for single spacing
%\usepackage{setspace} 
%\doublespacing


% Text layout
\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm 
\textheight 21cm

% Bold the 'Figure #' in the caption and separate it with a period
% Captions will be left justified
\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

% Use the PLoS provided bibtex style
\bibliographystyle{plos2009}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother


% Leave date blank
\date{}

\pagestyle{myheadings}
%% ** EDIT HERE **


%% ** EDIT HERE **
%% PLEASE INCLUDE ALL MACROS BELOW

%% END MACROS SECTION

\begin{document}

% Title must be 150 characters or less
\begin{flushleft}
{\Large
\textbf{Researcher perspectives on Data Publication and Peer Review}
}
% Insert Author names, affiliations and corresponding author email.
\\
John Ernest Kratz$^{1}$, 
Carly Strasser$^{1}$, 
\\
\bf{1} California Digital Library, University of California Office of the President, Oakland, CA, USA

$\ast$ E-mail: Corresponding John.Kratz@ucop.edu
\end{flushleft}

% Please keep the abstract between 250 and 300 words
\section*{Abstract}

Data ``publication'' attempts to appropriate for data the prestige of publication in the scholarly literature. 
While the scholarly communication community substantially endorses the idea, it hasn't fully resolved what a data publication should look like or how data peer review should work. 
To contribute an important and neglected perspective on these issues, we surveyed ~250 researchers across the sciences and social sciences, asking what expectations``data publication'' raises and what features would be useful to evaluate the trustworthiness and impact of a data publication and the contribution of its creator(s).  
We found that expectations of data publication center on availability.
Most respondents did not expect published data to be peer-reviewed, but peer review was the significant source of trust and value for a published dataset.


\section*{Introduction}

% Results and Discussion can be combined.
\section*{Results}
To find out what data publication and peer review means to researchers, we conducted an online survey in January and February of 2014.


\subsection*{Demographics}
We received 281 unique responses to the survey.
We analyzed the responses of 249 (81\%) of these who were deemed to be active researchers (summarized in Table 1).
The largest response was from biologists (37\%), followed by Archaeologists (13\%), Social scientists (13\%), and Envirnomental scientists (11\%).
Most (85\%) of the respondents were affiliated with academic institutions; 90\% of those are research-focused (rather than teaching).
Respondents covered the academic career spectrum: 41\% are principal investigators/lab heads, 24\% postdocs, and 16\% grad students.
We saw surprisingly few significant differences in response between disciplines or roles, so the results are presented in aggregate.
Because these policies are specific to the 

\subsection*{Background knowledge}

We asked a number of questions to gauge respondents engagement and familiarity with issues around data sharing and publication.
Respondents were asked to rate their familiarity with three United States (US) federal government policies related to data sharing and availability.
Because the policies are specific to the US, we restricted our analysis to the 79\% (197) of repondents who work in the US.
The best known was the National Science Foundation (NSF)'s Data Management Plan requirement\cite{national_science_foundation_gpg_2011}.
Less than half of the respondents had heard of the United States Office of Science and Technology Policy (OSTP) Open Data Initiative\cite{obama_making_2013} at all; this may be because the directive has not had any concrete effect on researchers yet, even though it will eventually affect virtually everyone doing research in the US.
On the other hand, even though the National Institutes of Health (NIH) data sharing policy\cite{national_institutes_of_health_final_2003} was enacted 11 years ago, only 4 (5\%) biologists claimed to `know all the details,' while 18 (24\%) had `never heard of it.'

Because data papers are a rapidly proliferating form of data publication that has been gaining attention, we asked about data journals specifically.
We provided a free text box and asked respondents to list any data journals that they could name.
Only 16\% of respondents named a data journal; \emph{Ecological Archives} was the most frequently named, by 16 respondents. 
The second most common response was Nature Publishing Group (NPG)'s \emph{Scientific Data} (named by 14), even though it had not started publishing at the time of the survey.
\emph{Earth System Science Data (ESSD)} (7), \emph{Biodiversity Data Journal} (6) and \emph{Geoscience Data Journal} (5) followed.
A number of respondents listed non-journal data publishers: figshare (6), Dryad (3), and Zenodo(1).

\subsection*{Data sharing mechanisms}
One of the goals of this survey was to explore the distinction between data sharing and publication.
Unlike data publication, a relatively new and unfamiliar concept to researchers, most have experience with and opinions about data sharing.
Many respondents (56\%, 140) said that it is very important to share the data that underlies a study; we did not see any statistically meaningful difference between disciplines (\chi^{2}= 39.1, p= 0.18). %Check that these values makes sense!
A majority (68\%, 168) have shared data in the past.
Of those, 58\% (98) saw their data reused by someone, and 62\% (61) of those reuses led to publication.
Slightly fewer, 61\% (151) had reused someone else's data; 69\% (104) of these led to a publication.

Because the channel of sharing is generally considered to be a crucial feature of data publication <CITE>, we asked about sharing channels (Figure 2).
We asked how respondents shared data, how others got their data, and how they got others data.
We provided four answers that correspond to the four methods for external data sharing that emerged from interviews by Kim\cite{kim_institutional_2012}: email/direct contact, database or repository, and journal website.
In all cases, email/direct contact was the most frequently reported channel for sharing; 87\% (146) shared data that way, 82\% (82) reported that others got their data that way, and 57\% (86) reused data that they got that way.
As a fifth alternative, 5\% (8) respondents wrote-in that they had extracted data from the text, tables, or figures of a published paper.

\subsection*{Data sharing credit}
A primary goal of data publication is to enable researchers who do the work needed to make data reusable to get credit for their work.
We wanted to assess what the researchers taking our survey thought was appropriate credit for the creators of a dataset.
We asked how a researcher who shares data should be credited.
Formal citation in the reference list was the most common answer, given by 83\% (126)
Acknowledgement also ranked highly at 62\% (93)
The most common actual practice observed by examining published articles, informal reference in the body of the paper\cite{sieber_not_1995, mooney_anatomy_2012}, ranked lowest at 16\% (24).
Respondents who used the provided free-text field overwhemlingly wrote some variant on ``it depends.''
Two factors were cited: the publication status of the dataset (e.g. ``depends on whether the data is already published'') and the role of the data in the paper (e.g. ``authorship if data is [the] primary source of analysis, otherwise acknowledgment'').
Becase previous studies identified substantial differences in culture between disciplines\cite{harley_assessing_2010, swan_share_2008}, we tested for significant differences between disciplines (omitting Mathematics because the \emph{n} was too low for chi-square), but did not find any (\chi^{2}= 15.6, p= 0.9).

We also asked the subset who had published with someone else's data how they actually credited the creators.
The answers to this question corresponded well with the prior theoretical question.
Formal citation was the most frequently reported, by 63\% (81), followed by acknowledgment (LOOK UP).
No one admited to citing data informally in the body of the text.

Fear of `data vultures' appropriating and profiting from one's data without adequately acknowledging the source is a significant concern for researchers <CITATION NEEDED>, but we wanted to know how significant the problem really is.
To address this question, we asked respondents whose data had been reused in a publication how they felt about the amount of credit given.
Most of the 86 who answered the question, 63\% (54) felt that they had been credited appropriately, and a combined 78\% (67) thought they recieved appropriate or excessive credit.
This left 22\% (13) who felt that they recieved insufficent credit; only 2 (2\%) felt that the credit was `very insufficent.'
We considered the possibility that differences in satisfaction were driven by different attitudes toward what constitutes appropriate credit.  
To that end, we collapsed responses into three categories (insufficient, appropriate, and excessive), and tested for independence with each of the four specified answers in data sharing credit.
The strongest relationship, between less satisfaction and ``authorship on paper,'' was still far from statisctially significant (\chi^{2}= 3.26, p= 0.20, corrected \alpha= 0.0125).

\subsection*{Definitions}

Because the application of ``publication'' and ``peer review'' to data is largely based on the familiarity of these ideas to researchers, we wanted to know what ``data publication'' and ``peer review of data'' would actually mean to researchers.
We decomposed the prevalent models of data publication into a set of separate features and asked which would be expected to distinguish a ``published'' dataset from a ``shared'' one (Figure XXXA).
Not surprisingly, the most common expectations relate to access: 68\% (166) expect a published dataset to be openly available and 54\% (133) expect it to have been deposited in a repository.
More researchers expected a published dataset to accompanied by a traditional publication (43\%, 105) than a data paper (22\%, 55).
Perhaps surprisingly, only a 29\% (70) minority expect published data to have been peer reviewed.

To find out what researchers expected from data peer review, we asked what they would expect peer review of data to entail and provided a list of considerations that various organizations take into account when evaluating/validating datasets (Figure XXXB).
The most common responses were ``collection and processing methods were evaluated'' (90\%, 220) and ``descriptive text is thorough enough to use or replicate the dataset'' (80\%, 196).
The high rank of the latter suggests an awareness of the importance of metadata.
Fitting with current practice, there was little (22\%, 53) expectation that peer review would consider the novely or potential impact of the dataset.
An expectation that data publication includes peer review correlated postively (by Fisher exact test) with thorough descriptive text (OR= 5.67, p= 0.00029), plausibility considered (OR= 2.29, p= 0.0087), and novelty/impact considered (OR= 2.42, p= 0.0092), although only the first edges over an \alpha= 0.05 signficance threshold after applying the Bonferroni correction for multiple hypothesis testing.

We were interested in whether selections of items in each definition represented coherent ideas about publication or peer review, so we tested agains the null hypothesis that each item was selected (or not) independently of every other.
Not surprisingly, this hypothesis was rejected resoundingly for both data publication (\chi^{2}= 290.0, p= 6.9e-58) and peer review (\chi^{2}= 343.0, p= 5.7e-72)


\section*{Discussion}





% You may title this section "Methods" or "Models". 
% "Models" is not a valid title for PLoS ONE authors. However, PLoS ONE
% authors may use "Analysis" 
\section*{Methods}
\subsection*{Survey design and distribution}
All results were drawn from a 34-question survey officially open online from January 22nd to February 28 of 2014; two late responses recieved in March were included in the analysis.
The University of California, Berkeley Insitiutional Review Board approved the survey.
The survey could be completed anonymously.
Respondends affiliated with the Univeristy of California had an opportunity to supply an email address for help with data publication, but that information was not used in any way for the purposes of this article.

The survey asked questions in 3 categores: demographics, data sharing experience, and data publication perceptions.
Data publication perceptions consisted of ``mark all that apply'' questions about the definition of data publication and peer review and Likert scale questions about the value of various possible features of a data publication.

The survey was designed with a minimum of required questions.
Some pages were displayed dynamically based on answers to certain questions; therefore, n varies considerably.
The survey was administered as a Google Form.
Solicitations were distributed via social media (Twitter, Facebook, Google+), emails to listservs, and a blog post on Data Pub\cite{kratz_data_2014}.

\subsection*{Data processing and analysis}
Although opinions are unlikely to be controversial, light anonymization was performed prior to analysis.
University of California (UC) affiliation and questions with specific application to UC researchers were redacted.
Respondent locations were grouped into United States and ``Other''; this distinction was preserved because some questions asked about US government policies.  
Sub-disciplines with fewer than three respondents were merged into the corresponding discipline.
Listed data journals were standardized by hand.
Free text answers were replaced with ``Other.''

After anonymization, reponses were filtered for analysis.
Because the goal of the survey was to learn about researchers, we exempted from analysis anyone who self-identified as a librarian or information scientist.
To restrict the analysis to active researchers, we filtered anyone who affirmed that they had not generated any data in the last five years; the 90 respondents who did not answer this question were retained.
Finally, we exempted anyone who did not posess at least a Bachelor's Degree.
In total, 32 respondents were exempted from analysis, some on the basis of multiple criteria.

Analysis was performed using IPython\cite{perez_ipython_2007}, Pandas\cite{mckinney-proc-scipy-2010}, and Numpy\cite{van_der_walt_numpy_2011}.
Graphs were prepared using Python and formatted with Adobe Illustrator.
For significance testing, the Fisher exact test was used for 2x2 tables and contingency table chi-square for all larger tables.
A significance cutoff of \alpha=0.05 was used, corrected for multiple hypothesis testing when appropriate.
Because few questions were required, questions with no reply at all were considered to be skipped rather than an intentional ``none of these''.


% Do NOT remove this, even if you are not including acknowledgments
\section*{Acknowledgments}


%\section*{References}
% The bibtex filename
\bibliography{SurveyPaper}

\section*{Figure Legends}
%\begin{figure}[!ht]
%\begin{center}
%%\includegraphics[width=4in]{figure_name.2.png}
%\end{center}
%\caption{
%{\bf Bold the first sentence.}  Rest of figure 2  caption.  Caption 
%should be left justified, as specified by the options to the caption 
%package.
%}
%\label{Figure_label}
%\end{figure}


\section*{Tables}
%\begin{table}[!ht]
%\caption{
%\bf{Table title}}
%\begin{tabular}{|c|c|c|}
%table information
%\end{tabular}
%\begin{flushleft}Table caption
%\end{flushleft}
%\label{tab:label}
% \end{table}

\end{document}
