% Template for PLoS
% Version 1.0 January 2009
%
% To compile to pdf, run:
% latex plos.template
% bibtex plos.template
% latex plos.template
% latex plos.template
% dvipdf plos.template

\documentclass[10pt]{article}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}
% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

\usepackage{color} 

% Use doublespacing - comment out for single spacing
%\usepackage{setspace} 
%\doublespacing


% Text layout
\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm 
\textheight 21cm

% Bold the 'Figure #' in the caption and separate it with a period
% Captions will be left justified
\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

% Use the PLoS provided bibtex style
\bibliographystyle{plos2009}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother


% Leave date blank
\date{}

\pagestyle{myheadings}
%% ** EDIT HERE **


%% ** EDIT HERE **
%% PLEASE INCLUDE ALL MACROS BELOW

%% END MACROS SECTION

\begin{document}

% Title must be 150 characters or less
\begin{flushleft}
{\Large
\textbf{Researcher perspectives on data publication and peer review}
}
% Insert Author names, affiliations and corresponding author email.
\\
John Ernest Kratz$^{1}$, 
Carly Strasser$^{1}$, 
\\
\bf{1} California Digital Library, University of California Office of the President, Oakland, CA, USA

$\ast$ E-mail: John.Kratz@ucop.edu
\end{flushleft}


% Please keep the abstract between 250 and 300 words
\section*{Abstract}

Data ``publication'' seeks to appropriate the prestige of authorship in the peer-reviewed literature as a reward for creators of a useful and well documented datasets. 
Data publication has been embraced as a means to promote data sharing and numerous initiatives are underway, but the conversation in the scholarly communication community around what a data publication should be and how data peer review should work is ongoing. 
To contribute an important and neglected perspective, we surveyed ~250 researchers across the sciences and social sciences, asking what expectations``data publication'' raises and what features would be useful to evaluate the trustworthiness and impact of a data publication and the contribution of its creator(s).  
We found that expectations of data publication center on availability.
Most respondents did not expect published data to be peer-reviewed, but peer review was the significant source of trust and value for a published dataset.


\section*{Introduction}

\subsection*{Data sharing}

In 1985--- almost 30 years ago--- Stephen Ceci surveyed 847 scientists and concluded, ``it is clear that scientists in all fields endorse the principle of data sharing as a desirable norm of science''\cite{ceci_scientists_1988}.
This endorsement has not weakened in the intervening decades; the importance of data sharing was affirmed in 2010 by more than 65\% of faculty at California Polytechnic State University (Cal Poly)\cite{scaramozzino_study_2012} and in 2013 by 94\% of researchers surveyed by the Expert Advisor Group on Data Access (EAGDA) in the United Kingdom (UK)\cite{bobrow_establishing_2014}.
The respondents to the 1985 survey, which was published in 1988, advocated data sharing ``to allow replication and extension of one's own findings''\cite{ceci_scientists_2014}, and data sharing is motived in general by the twin affordances of reproducibility and (re)use.
The reproducibility problem plaguing science in the scholarly\cite{ioannidis_why_2005, prinz_believe_2011, mobley_survey_2013} and mainstream\cite{zimmer_rise_2013, _how_2013, _trouble_2013} press could be addressed, in part, by opening data to scrutiny\cite{drew_lost_2013, collins_policy_2014}.
Beyond reproducing previous analysis, reuse of existing data cuts research costs\cite{piwowar_data_2011} and enables questions to be addressed that wouldn't be possible otherwise\cite{stewart_meta-analysis_2009, borenstein_introduction_2011}.
Despite enthusiasm for data sharing in principle, Ceci alleges that in practice, ``something is amiss in the academy.''

Researchers frequently don't make data available, even when they support the idea or are theoretically obliged to do so.
Alsheikh-Ali et al. (2011) examined 351 articles and found that 59\% did not adhere to the data availability requirements of the journal where they were published.
Vines et al. (2014) requested data from 516 articles published between 1991 and 2011 and got it less than half (47\%) of the time\cite{vines_availability_2014}
Researchers themselves agree that this is a problem.
In 1985, 59\% of scientists surveyed by Ceci complained that their colleagues were disinclined to share data\cite{ceci_scientists_1988}
Fifteen years later, 67\% of respondents to an international survey by the Data Observation Network for Earth (DataONE) agreed that ``[l]ack of access to data generated by other researchers or institutions is a major impediment to progress in science'' and 50\% felt that their own research had suffered\cite{tenopir_data_2011}.
That same year, 65\% Cal Poly faculty claimed that data sharing is important, but less than half of those followed through with their own data\cite{scaramozzino_study_2012}.

Why do even researchers who believe that data sharing is important not do it?
Previous surveys unearthed a number of reasons: concern about the ethical and legal issues encrusting human subject data, mistrust of that others have the expertise to use the data appropriately, hope of wringing more articles from the dataset, and fear that the data will be ``stolen'' without credit or acknowledgment.
Researchers expressed ethical concerns in reports from the Research Information Network (RIN) in 2008 and EAGDA in 2014; in the 2014 report, this was the second most frequently mentioned constraint (by 55\% of respondents)\cite{swann_share_2008, bobrow_establishing_2014}.
The risk of losing publications from premature sharing came up in 60\% of a 2012 series of interviews of United States (US) scientists, more than any other risk; fear of ``data theft'' was mentioned in 32\% of the interviews\cite{kim_institutional_2012}.
However, by far the most consistent reason given is that preparing and documenting data to a high enough standard to be useful just takes too much time.

In the UK, RIN report described lack of time as a major constraint\cite{swan_share_2008} and in the EAGDA report, it was the most frequently mentioned constraint (by 66\% of respondents)\cite{bobrow_establishing_2014}.
Time was the most often mentioned (by 44\%) unavoidable cost (as opposed to potential risk) to data sharing in Kim and Stanton's survey\cite{kim_institutional_2012}.
It was the most frequent reason for not sharing data in the multidisciplinary DataONE survey (by 54\% of respondents)\cite{tenopir_data_2011} and the second most frequent (after the perception that no one needs the data) in a follow-up survey of Astrobiologists\cite{aydingolu_data_2014}.
Time investment was the second most frequent (after loss of control) objection to data sharing in a 2012 survey of biodiversity researchers\cite{enke_user's_2012}.
Although the process of preparing and documenting data for sharing to be shared openly could undoubtedly be streamlined with better planning, education, and tools, it will always take time and effort.
The underlying problem is that this effort is not rewarded.
Lack of acknowledgment was the third most popular objection in the biodiveristy survey, and ~2/3 of respondents would be more likely to share if they were recognized or credited when their data is used.
In the EAGDA report, 55\% of respondents said that lack of tangible recognition and rewards constrains data sharing, and at least 75\% of respondents felt that the UK Research Excellence Framework (REF) does not recognize data to some or great extent relative to publications, but that it should.
The need to compensate researchers who share data with scholarly prestige is a major driver of the movement toward data publication.
 
\subsection*{Data publication}
 
Data publication appropriates terminology (``publication'', ``peer review'') from the scholarly literature to integrate data into the academic reward system\cite{costello_motivating_2009, lawrence_data_2011, atici_other_2012}.
The model of data publication that most closely mimics the existing literature is the data paper.
Data papers describe datasets, including the rationale and collections methods, without offering any analysis or conclusions\cite{newman_data_2009, callaghan_processes_2013}.
Data papers are colonizing existing journals like \emph{F1000Research} and \emph{Internet Archaeology} and new dedicated journals such as \emph{Earth System Science Data}, \emph{Geoscience Data Journal}\cite{allan_geoscience_2014}, and Nature Publishing Group's \emph{Scientific Data}--- which describes itself concisely as ``a publication venue that credits scientists who share and explain their data." \cite{_more_2014}
Data papers are inevitably peer-reviewed based on the dataset; its description; and whether the two form a complete, consistent, and useable package\cite{lawrence_data_2011}.
The appeal data papers is straightforward: they are unquestionably peer-reviewed papers, so academia knows how (if perhaps not how much) to value them.

However, data papers are by no means the only approach to data publication.
Data publishers include repositories such as Dryad(http://www.datadryad.org/), figshare(http://figshare.com/), and Zenodo(http://zenodo.org/) where researchers can self-deposit any kind of research data with light documentation requirements and minimal validation; Dyrad requires that the data be associated with a ``reputable'' publication, while figshare and Zenodo are completely open.
Domain-specific repositories frequently have more stringent documentation requirements and access to the domain knowledge needed thorough evaluation.
For instance, The National Snow and Ice Data Center (NSIDC) evaluates incoming data in a complex process that incorporates both internal technical expertise and external domain knowledge\cite{weaver_data_2012}.
As a final example, Open Context publishes carefully processed and richly annotated archaeology data, some of which passes through editorial and peer review\cite{kansa_we_2014}.
One thing that all of these publisher have in common is that they endeavor to make datasets formally citable (in part through assignment of stable identifiers) as a means to credit the creators.

The variety of forms of data publication attests to a general shortage of consensus on what, exactly, it means to publish data.
Noting a lack of both consensus and interest on the part of researchers, the RIN report of 2008 adopted a deliberately minimal definition, ``making datasets publicly available''\cite{swan_share_2008}.
While eminently practical, this definition does not do much to distinguish publication from sharing (except for ruling out certain channels) or to advance its prestige.
More recently, Callaghan et al. (2012) suggesting distinguishing between shared (available), \underline{p}ublished (also citable), and \underline{P}ublished (also peer-reviewed) data\cite{callaghan_making_2012}.
We have argued that the consensus in the scholarly communication community is that published data is openly available and citable, but that what kind of validation (if any) is required to qualify is still an open question\cite{kratz_data_2014}.
It is easy to forget, however, that what data publication means to the scholarly communication community is substantially irrelevant.
The point of calling data that has been made public through whatever particular process ``published'' is to exploit the meaning of the word to researchers; the important definition is what data publication means to them. 

Researchers have been surveyed about data sharing many times this decade, but not about data publication\cite{harley_assessing_2010, westra_data_2010, tenopir_data_2011, kim_institutional_2012, scaramozzino_study_2012, williams_gathering_2013, bobrow_establishing_2014, strasser_dataup_2014}
The RIN report of 2008 is the last survey to ask researchers about data publication; while the conclusions are undeniably valuable, as mentioned above, it uses the term broadly, such that it is difficult to make any distinction between attitudes about sharing data and about publishing it\cite{swan_share_2008}.
Consequently, open questions abound:
What would a researcher expect data publication to mean?
What about peer review of a dataset?
Do current models satisfy those expectations?
What potential features of a data publication would be useful for evaluating the quality of the data?
For evaluating the contribution of the creator(s)?
To get this critical perspective we conducted an online survey of active researcher perceptions of data publication.


\section*{Results}

\subsection*{Demographics}

We collected responses to an online survey of data publication practices and perceptions in January and February of 2014 and received 281 unique responses.
Because we distributed the survey solicitation via social media and email listservs and did not contact most recipients directly, we cannot estimate with any accuracy how many researchers received the solicitation or calculate a response rate.
Our analysis was restricted to the 249 (81\%) respondents who we deemed to be active researchers (summarized in Table 1).
Researchers from 20 countries responded, but most ($79\%, n=197$) were affiliated with institutions in the US.
The institutions were largely academic ($85\%, n=204$); 94\% ($n=191$) of those were focused on research rather than teaching. 
The largest response was from Biologists (37\%), followed by Arch{\ae}ologists (13\%), Social scientists (13\%), and Environmental scientists (11\%).
We heard from researchers across the academic career spectrum: 41\% ($n=102$) were principal investigators/lab heads, 24\% ($n=61$) postdocs, and 16\% ($n=41$) grad students.
We saw surprisingly few significant differences in responses between disciplines or roles, so we have generally presented the results in aggregate.
For breakdown of data by discipline or role, see the full raw (except for redactions to preserve anonymity) dataset at XXX. %fill in data citation

\subsection*{Background knowledge}

We asked a number of questions to assess engagement and familiarity with data sharing and publication (Figure \ref{Figure_policy_knowledge}).
Respondents rated their familiarity with three US federal government policies related to data sharing and availability.
Because the policies are specific to the US, we restricted our analysis to the respondents who work in the US.
Respondents were most familiar with the National Science Foundation (NSF)'s Data Management Plan requirement\cite{national_science_foundation_gpg_2011}.
Less than half had heard of the United States Office of Science and Technology Policy (OSTP) Open Data Initiative\cite{obama_making_2013} at all; although the directive will eventually affect virtually all researchers who receive US government funding, awareness is most likely low because concrete policies have not been implemented yet. 
A much older policy, the National Institutes of Health (NIH) data sharing policy\cite{national_institutes_of_health_final_2003} was enacted 11 years ago, but only 4 biologists ($5\%$) claimed to know all the details, fewer than the 18 ($24\%$) who had never heard of it.

The rapid proliferation of data journals led us to ask about them specifically.
Respondents listed any data journals that they could name in a free text box.
Only 16\% ($n=40$) of respondents named any data journals. 
\emph{Ecological Archives} was the most frequently named, by 16 respondents. 
The second most frequent response was Nature Publishing Group's \emph{Scientific Data} (named by 14), even though it had not started publishing at the time of the survey.
\emph{Earth System Science Data (ESSD)} ($n=7$), \emph{Biodiversity Data Journal} ($n=6$) and \emph{Geoscience Data Journal} ($n=5$) followed.
A number of respondents listed non-journal data publishers: figshare ($n=6$), Dryad ($n=3$), and Zenodo($n=1$).

\subsection*{Data sharing mechanisms}

Data publication is a relatively new and unfamiliar concept to researchers, but most do have experience with and opinions about data sharing, and we explored those before moving on to publication.
Many respondents ($56\%, n=140$) said that it is very important to share the data that underlies a study; differences between disciplines were not statistically meaningful ($\chi^{2}= 39.1, p= 0.18$). %Check that these values makes sense!
Most have experience sharing data ($68\%, n=168$) or reusing shared data ($61\%, n=151$).
Of the researchers who shared, 58\% ($n=98$) saw their data reused by someone and 62\% ($n=61$) of those reuses led to a published paper.
Most of the reusers published a paper with the data ($69\%, n=104$).

Because the channel of sharing is a crucial feature of data publication\cite{kratz_data_2014} we asked researchers with data sharing experience about the medium of transmission (Figure 2).
The supplied answer choices were the four methods for external data sharing that emerged in interviews by Kim and Stanton\cite{kim_institutional_2012}: email/direct contact, personal website, journal website, and database or repository.
Email/direct contact was the most frequently reported method for sharing; 87\% ($n=146$) shared data that way, 82\% ($n=82$) of other researchers obtained the respondent's data that way, and 57\% ($n=86$) of the respondents who reused data got it way.
Eight respondents (5\%) wrote-in an analog channel we that had not considered; they extracted data from the text, tables, or figures of a published paper.

\subsection*{Data sharing credit}

Crediting data creators is a primary concern in data publication, so we asked how a dataset creator should be credited by a reuser (Figure 3A).
Additionally, we asked those who had published with shared data how they actually credited the creator (Figure 3B).
The most common answer to the theoretical question, from 83\% ($n=126$) of respondents, was formal citation in the reference list.
Acknowledgement also ranked highly at 62\% (93).
Most respondents who gave a free-text answer wrote some variant on ``it depends.''
They cited two factors: the publication status of the dataset (e.g. ``depends on whether the data is already published'') and the role of the data in the paper (e.g. ``authorship if data is [the] primary source of analysis, otherwise acknowledgment'').
Because previous studies reported differences in culture between disciplines\cite{swan_share_2008, harley_assessing_2010, tenopir_data_2011}, we tested whether discipline had a significant effect on the response  (omitting Mathematics because of low $n$) and found that it did not ($\chi^{2}=15.6, p=0.9$).
Reported practice fit well with theory: formal citation was the most popular method ($63\%, n=81$), followed by acknowledgment ($50\%, n=70$).
There was, however, a statistically significant ($\chi^{2}=15.8, p=0.0013$) difference between answers to the two questions.
The likely source of this distinction is that while a a few respondents ($16\%, n=24$) felt that it was appropriate to cite data informally in the body of the text, none said that they had actually done it.
In direct contrast, study of the literature shows that informal citation is actually the most common approach\cite{sieber_not_1995, mooney_anatomy_2012}.

Many researchers fear that shared data might be used by ``data vultures'' who contribute little and don't acknowledge the source\cite{kim_insitutional_2012}.
To assess whether this fear is well founded, we asked respondents whose data had been reused for a publication whether they were adequately credited (Figure 3C).
Most ($63\%, n=54$) felt appropriately credited, and a combined 78\% ($n=67$) felt the credit was appropriate or excessive.
This left 22\% ($n=13$) who were unsatisfied; only 2 (2\%) felt that the credit was ``very insufficient.''
These differences in satisfaction could derive from different attitudes toward appropriate credit.  
To test this, we collapsed responses into three categories (insufficient, appropriate, and excessive), and tested for independence with each of the four specified answers in data sharing credit.
The strongest relationship, between less satisfaction and belief that authorship is appropriate, was still far from statistically meaningful ($\chi^{2}= 3.26, p= 0.20$, corrected $\$alpha= 0.0125$).

\subsection*{Definitions}

The central question we hoped to answer is what ``data publication'' and ``data peer review'' actually mean to researchers.
We decomposed the prevalent models of data publication into a set of potential features and asked respondents what would distinguish a ``published'' dataset from a ``shared'' one (Figure 4A).
The most prevalent expectations relate to access: 68\% ($n=166$) expect a published dataset to be openly available somehow and 54\% ($n=133$) expect it to have been deposited in a repository or database.
Substantially more researchers expected a published dataset to be accompanied by a traditional publication ($43\%, n=105$) than by a data paper ($22\%, n=55$).
Only a minority of 29\% ($n=70$) expected published data to have been peer reviewed.

Much of the prestige of scholarly publication derives from having survived the peer review process.
It is natural, then, that many data publication initiatives model their validation process on peer review and use the term for its prestige and familiarity.
However, it is not obvious exactly how literature peer review should be adapted for data or what guarantees it should make.
To learn what researchers expect from data peer review, we provided a selection of considerations that data peer reviewers might take into account (Figure 4B).
The most common responses sidestepped examination of the data itself; 90\% ($n=220$) of respondents expected evaluation of the methods and 80\% ($n=196$) of the documentation.
%insert something about evaluation of the data
There was little ($22\%, n=53$) expectation that data reviewers would consider novelty or potential impact.
We were interested to know whether respondents who expected data publication to include peer review and those who did not had different ideas about what data peer review would entail.  
Expectation that data publication includes peer review was positively correlated with consideration of documentation ($OR= 5.67, p=0.00029$), plausibility ($OR= 2.29, p=0.0087$), and novelty/impact ($OR= 2.42, p= 0.0092$), but only the first is statistically significant at the $\alpha= 0.05$ level after correcting for multiple hypothesis testing.
%again, figure out how to boil this down to a single number

We were interested in whether respondents selected items to answer the above questions independently or as coherent constellations of ideas, so we tested for independence of items within each definition. 
Not surprisingly, there is strong evidence for relationships between items within both data publication ($\chi^{2}=290.0, p=6.9\times10^{-58}$) and peer review ($\chi^{2}= 343.0, p= 5.7\times10^{-72}$).
To identify relationships between specific items, we performed pairwise Fisher exact tests between each feature (Figure 5).
Within data publication, we found a dense set of statistically significant (at the $\alpha=0.05$ level, corrected to $\alpha=0.0018$) associations between items related to access and preservation.
For example, repository deposit was linked to openly availability ($OR=9.55, p=7.6\times10^{-14}$), assignment of unique identifier, ($OR= 3.33, p= 1.33\times10^{-5}$), and both formal ($OR= 4.49, p= 3.94\times10^{-6}$) and rich ($OR= 3.83, p=1.14\times10^{-6}$) metadata.
Formal and rich metadata were themselves linked ($OR= 12.5, p=2.7\times10^{-14}$). 
Formal metadata was also linked to assignment of a unique identifier, which is sensible in that an identifier is meaningless without metadata ($OR= 7.92, p= 5.05\times10^{-11}$).
Another carrier for metadata, a data paper, was linked to both rich ($OR=3.64, p=4.22\times10^{-5}$) and formal ($OR=4.30, p=1.32\times10^{-5}$) metadata.
Data papers were the only item associated with peer review ($OR=3.00, p=0.0011$).
Traditional papers had no significant associations at all; the closest was with data paper ($OR=1.93, p=0.023$).

Potential considerations during peer review are also linked significantly with a corrected cutoff of $\alpha=0.0033$.
Three strong linkages formed a triangle from appropriate methods to standardized metadata ($OR=7.91, p=0.00081$) to technical evaluation ($OR=4.56, p=3.4\times10^{-6}$) and back ($OR=5.9, p=7.9\times10^{-5}$).
Plausibility correlated with other factors that require domain expertise: appropriate methods ($OR=4.51, p=0.0014$), adequate documentation ($OR=4.87, p=2.5\time10^{-6}$), and novelty/impact ($OR=5.50, p=1.1\times10^{-5}$).
Plausibility was the only association for novelty/impact.

\subsection*{Values}

Validation of published data facilitates reuse only if potential reusers trust the means of assessment.
To learn what indicators of quality researchers trust, we presented respondents with four possible indicators and asked how much to rate how much confidence each would confer.
All four inspired at least some confidence in most researchers (ranging from 89\% to 98\%).
Respondents trusted peer review above all else; 72\% ($n=175$) said it conferred high or complete confidence and only 2\% ($n=4$) would feel little or no confidence.
The second most trusted indicator was knowledge that a traditional paper had been published with the data; 56\% ($n=137$) would have high or complete confidence.
Reuse of the data by a third party came in third, with 43\% ($n=106$). 
Description by a data paper was the least convincing at 37\% ($n=89$) high or complete confidence, although reuse inspired little or no confidence in more respondents ($11\%, n=25$).
%could try to relate peer review confidence to definition, but the results aren't that clear / don't make that much sense

Beyond reuse, data publication should enable researchers who create useful datasets to recieve credit.
To that end, we asked what metrics researchers would most respect when evaluating a dataset's impact.
Respondents considered number of citations to be the most useful metric; 49\% ($n=119$) found citation count highly or extremely useful. 
Less expectedly, a subtantial 32\% ($n=77$) felt the same way about number of downloads.
The distinction between citation and download counts shrinks to 9\% if the comparison is made at the level of at least somewhat useful (82\% versus 73\%).
Only a minority of respondents considered search rank ($42\%, n=102$) or altmetrics ($37\%, n=91$) to be even somewhat useful.

Even before quality or impact enter consideration, the prestige associated with publishing a dataset is influenced by its format.
We distilled a multiplicity of data publications formats to four generic modelsâ€“- with or without a data paper and with or without peer review-- and asked respondents how heavily they would weight each model if it appeared on a reseacher's curriculum vit{\ae}.
As a point of comparison, respondents also rated the value of a traditional paper; 60\% ($n=145$) give one a great deal of weight and another 36\% ($n=87$) give it significant weight.
The most valuable data publication model was data published with a peer-reviewed data paper, but even that was only given a great deal of weight by 10\% ($n=23$), although another 46\% ($n=109$) gave it significant weight.
A peer-reviewed dataset with no paper dropped to 5\% ($n=12$) giving a great deal of weight, while an un-peer-reviewed data paper dropped to 1\% ($n=2$).
Thus, peer review outweighed having a data paper as a factor. 
A substantial, 27\% ($n=65$) would award an un-peer reviewed dataset no weight at all.
For this question, which explicitly addressed evaluation of dataset creators, we were particularly interested in the 26\% (59) of survey respondents who had experience on a tenure and promotions committee.
We compared their responses with those who had not served on a committee by chi square.
Values ranged from a high of $\chi^{2}= 7.427$ ($p= 0.12$) for peer-reviewed datasets (valued slightly less by experienced respondents) to $\chi^{2}= 2.097$ ($p= 0.35$) for traditional papers (valued slightly more by experienced respondents); for 5 comparisons, $\alpha=0.01$, and none can be considered statistically significant.
% just do one chi square for the whole thing?

\section*{Discussion}

\subsection*{Demographics}

Although this survey was international in scope, most of the respondents are affiliated with institutions in the United States.
In this respect, these respondents (84\% North American) resemble those of the DataONE survey\cite{tenopir_data_2011} (73\% North American) and not those of the in-depth reports prepared by EAGDA\cite{bobrow_establishing_2014} and the RIN\cite{swan_share_2008}, which were carried out in the UK.
A single assessment framework, the Research Excellence Framework (REF), is critical to most researchers, creating a significantly different environment.
At least 75\% of respondents in the EAGDA report felt that the REF does not recognize data to some or great extent relative to publications, but that it should\cite{bobrow_establishing_2014}.
The bulk of our responses (85\%) came from academic institutions, which is similar to DataONEs 81\%\cite{tenopir_data_2011}.  
Ceci's initial survey was academic\cite{ceci_scientists_1988}.
Scaramozzino's was conducted entirely at a single teaching-oriented university, California Polytechnic State University\cite{scaramozzino_study_2012}. 

Reseachers responded from all of the major roles in academia and a variety of disciplines.
In terms of role, our respondents again resemble those of the DataONE survey.
There, 47\% were professors and 13.5\% grad students; here, 41\% were principal investigators (PIs) and 16\% grad students\cite{tenopir_data_2011}..
Most other surveys were restricted to PIs.
An exception, the EAGDA survey, sill mostly (69\%) heard from PIs\cite{bobrow_establishing_2014}.
Our largest response was from Biologists (37\%), followed by Archaeologists (13\%), Social scientists (13\%), and Environmental scientists (11\%). 
DataONE heard mostly from researchers in its area of focus, Environmental sciences and Ecology (36\%), followed by Social science (16\%) and Biology (14\%)\cite{tenopir_data_2011}. 
Scaramozzino's survey included a high proportion of Physicists and Mathematicians, but 18\% of respondents were Biologists\cite{scaramozzino_study_2012}.
The EAGDA survery was heaviest in biomedical fields, such as Epidemiology and (26.8\%), Genetics/Genomics (20\%), but also featured 31.4\% Social scientists\cite{bobrow_establishing_2014}. 
Whereas DataONE uncovered statistically distinct data sharing attitudes between respondents in different roles and disciplines, we did not.
The significance of distinctions in one survey and not the other is at least as likely to be an artifact of the difference in $n$ as a reflection of real differences in the respondent populations.

As is the case for many of the previous surveys, participation was entirely voluntary and uncontrolled, and responses are potentially biased toward researchers with an interest in data sharing and publication.
However, the high proportion of respondents who did not name any data journals (84\%) and the low awareness of US federal policies (e.g., 35\% of US respondents had never heard of the NSF data management plan requirement and 62\% had never heard of the OSTP Open Data Initiative) suggests that our respondents are not atypically invested in these issues.
To provide context, only 40\% of respondents to the EAGDA survey were unaware of data journals.

\subsection*{Data publication}

The RIN report of 2008 concluded that ```publishing' datasets means different things to different researchers''\cite{swan_share_2008} and we found that little has changed.
Even the most frequently named defining feature in this survey, open availability, was only chosen by ~2/3 of respondents.
One respondent simply wrote in ``terms are confusing.''
However, the emergence of systematic relationships between some of the features demonstrates that the responses were not utterly confused.
We observed two, arugably three, independent concepts of data publication.

The most widely-held concept centers on present and future access.
Open availability tightly correlates with the second most frequent feature, repository deposit.
Repository deposit correlates with three other conceptually related features (unique identification, rich metadata, and formal metadata), numerous interconnections unite all five of these features.
This concept of publication maps well onto virtually all present data publication implementations, including lightweight approaches like figshare and Zenodo..

The second concept lingers from the pre-digital days of scholarly communication: published data is data that has been used or described in a traditional journal article. 
Nearly half (43\%) of the respondents chose ``basis of a research paper'' as a defining feature of data publication. 
Surprisingly, this and the previous concept did not compete significantly, but were instead almost completely independent.
The traditional paper concept reflects how researchers speak (e.g. to ``publish an experiment'' is to publish a research paper that uses the experiment), but does not match the conversation in the scholarly communication community, where data that had been used or described but not made available would not be considered to have been published.
This mismatach is clearly a potential source of confusion to be avoided.

The third concept, not entirely independent from the first, is that a published dataset has been described by a data paper.
Data papers correlate with peer review and both kinds of metadata, but not with features related to the disposition of the data (e.g. open availability or repository deposit), even though virtually all data paper publishers require repository deposit.
Researchers are still not widely familiar with data papers; ``described in a data paper'' was the least popular defining feature (22\%), and most respondents (84\%) did not name any data journals.
Data papers confered less trust than any other feature, but only by a small margin; 36\% of respondents derive high or complete confidence from a data paper, compared to 44\% from successful reuse.
Not surprisingly, respondents regarded data papers as much less valuable than traditional research papers; only 10\% valued a data paper a great deal. %60\% of respondents would give a traditional paper a great deal of weight, but only 10\% value a data paper that highly.
Data papers may become more valued as awareness spreads.
In reference to data journals, one respondent wrote in ``I've never heard of this, but it sounds fantastic.''
Alternately, research communities may conclude that data paper should be valued less.
Already, 55\% of respondents gave a data paper at least significant value, which may be appropriate.
Data papers clearly add percieved value to a dataset, but not as much as peer review.
 
\subsection*{Data validation}

Quality control via peer review is integral to traditional scholarly publication so it is no surprise that, in reference to data publication, the RIN noted ``[t]here is, for some, also an implication that the information has been through a quality control process''\cite{swan_share_2008}.
Even in regard to data, researchers trust the traditional scholarly publication process: our respondents trusted peer review and use in a research paper more than any other indicators of quality.
However, less than half expected a published dataset to have been used in a published research paper and only a third expected it to have been peer reviewed.
We conclude, with the RIN, that researchers don't have a clear idea what quality control to expect from published data.
In this respect, the research and scholarly communication communities are in perfect agreement.
How, and how extensively, to assess data quality is the least settled of the many open questions surrounding data publication, and different initiatives take a variety of approaches, including collecting user feedback, distinct technical and scienctific review, and closely modeling literature peer review\cite{kratz_data_2014}.

Peer review establishes the trustworthiness of dataset and elevates its percieved value more than any other factor in this survey.
This may, in part, reflect the high proportion of biologists, a discipline that values peer review especially highly\cite{harley_assessing_2010} % look at biologists v. everyone else
Despite one respondent's remark that ``I have never heard this term applied to a dataset and I don't know what it means,'' expectations of peer review were more consistent than of publication. 
Whereas the most popular defining feature of data publication was only chosen by ~2/3 of respondents, 90\% of respondents agreed that they expect data peer review to include evaluation of collection and processing methods.
In fact, fully half of the peer review assements were chosen by more respondents than the top data publication feature.
Unsurprisingly, a majority of respondents expect assessments that require domain expertise, i.e. review by peers in their field.
Assessment of plausibility was linked with three other assessment that require domain expertise: method evaluation, adequacy of metadata for replication, and potential novelty/impact.
The high (80\%) expectation that peer review of data includes peer review of its metadata suggests that researchers are aware of the critical importance of documentation for data reuse and replication.
That and the low (22\%) expectation that peer review consider novelty/impact are in line with current data journal peer review processes and guidelines\cite{kratz_data_2014}.
However, our survey question focused on the aspects of a data publication that might be assessed, not the review process; peer review expectations might be satisfied through any number of pre- or post-publication processes.
We conclude that models of data publication without peer review are unlikely to confuse researchers, but that peer review greatly enhances both reuse and reward.
Futhermore, assessment processes that at least meet the expectations of peer review will be critical for data publications to attain a status comparable to that of journal articles.

The idea that ``data use in its own right provides a form of review''\cite{parsons_data_2010} is frequently expressed in the conversation around data publication. 
Based on past experiences, we were surprised that successful reuse did not inspire more trust; both peer review and ``basis of a traditional paper'' inspired more confidence than reuse.
(Although it is worth nothing that serving as the basis of a research paper is iteself evidence of successful use, just not by a third party.)
However, respondents considered the the direct consequences of successful reuse--- citations--- to be the most useful metric for assessing value/impact.
This apparent contradiction could result from evaluating trustworthiness and impact differently or different concepts of  ``successful'' reuse and reuse that that results in a citation.
The combined value of enhancing trust and establishing impact makes tracking dataset citations emminently worthwhile, but still no substitute for peer review.

\subsection*{Credit for data}

The scholarly communication community agrees that data should be cited formally in the reference list\cite{force11_data_citation_synthesis_group_joint_2014}, but this is rarely actually done\cite{sieber_not_1995, mooney_citing_2011, mooney_anatomy_2012}.
In a 1995 survey of 198 papers that used published social science datasets, 19\% cited the dataset with at least the title in the reference list\cite{sieber_not_1995}.
A followup 17 years later found that only 17\% of papers meeting even this low standard, and therefore that practice has not improved\cite{mooney_anatomy_2012}.
The most common actual approach is informal citation in the methods or results section of the paper; 30.8\% of papers in 1995 and 69.2\% in 2012 included the dataset title somewhere in the text.
Notwithstanding this dismal state of practice, researchers agree that the correct approach is formal citation; 95\% of respondents to DataONE said that formal citation was a fair condition for data sharing, 87\% of astrobiologists said the same, and 71\% of biodiversisty researchers said they would like their data to be cited ``in the references like normal publications''\cite{tenopir_data_2011, aydingolu_data_2014, enke_user's_2012}.
Here too, formal citation was the most popular response to both how a dataset creator should be credited and how the respondent actually credited data creators.
No respondents admitted to citing data informally in the text.
This apparent disconnect between what is observed in the social science literature and self-reported practice could arise in any of a number or ways: it may be that social science is not a representative discipline, that occasions when respondents cited data formally are easier to bring to mind, or that researchers define dataset reuse differently than the authors of the literature surveys.
For instance, a biologist who uses a sequence from GenBank and mentions the asscession number in the methods section of the paper might not think of that activity as data reuse warranting a formal citation.
Beyond somewhat abstract notions of credit, formal data citations are useful to the 71\% of respondents to the EAGDA survey who already track use of their datasets ``through details of publications generated using the data''\cite{bobrow_establishing_2014}. 
In sum, we conclude that researchers are aware of the benefits of formal data citation and suggest that data ciation efforts focus on implementation rather than persuasion. 

While respondents deemed citation the most useful metric of dataset value, they also attached high value to download counts.
These preferences align with the practices reported in the EAGDA survey, where 43\% of respondents tracked downloads of their datasets\cite{bobrow_establishing_2014}
In the present scholarly communication infrastructre, repositories can count downloads much more easily than citations.
Citations are preferable, but downloads are the ``low hanging fruit'' of data metrics.
In comparison to download counts, appreciation of altmetrics (e.g. mentions in social media or the popular press) was low; only ~1/3 of respondents found them even somewhat useful in assessing impact.
Altmetrics for research articles are still being developed, so it is not surprising that researchers are unsure what they might signify for data.
For data publishers, there is certainly no harm in providing altmetrics--- and a majority of respondents did find them at least slightly useful--- but they are unlikely to have much impact in the short term.

While the biggest cost to sharing data is the time required, the biggest fear is that ``data vultures'' will strip the dataset for publications without adequately acknowledging the creator(s)\cite{kim_institutional_2012}.
To learn how well-founded these fears are, we asked respondents how satisfied they were with their credit the last time someone published using their data.
The majority felt that the credit was appropriate, but the fraction that felt shortchanged (22\%) is too large to ignore, and we must conclude that this dissatisfaction is a real problem.
Whether the problem is ultimately with the way dataset creators are credited or the way dataset creators expect to be credited is for researcher communities to decide.
We can say that there was no significant difference in how satisfied and dissatisfied respondents thought dataset creators should be credited, so the variability in satisfaction was most likely driven by variability in credit recieved rather than the recipient's attitude.
As data publication takes shape, the problem can be reduced by solidification of community norms around data use, increased prestiges for dataset creators, and better adoption of formal data citation.

\subsection*{Data publication redux}

The outcome of this survey offers some practical guidance for data publishers seeking to meet researcher expectations and enhance the value of datasets.
Above all else, researchers expect published data to be accessible, generally through a database or repository; this fits well with current practice and, indeed, with the idea of publication at its most fundamental.
The research and scholarly communication communities aggree that formal citation is the most appropriate way to credit a dataset creator, and a number of steps can be taked to encourage this practice.
Data publishers should enable formal citation (e.g. by assigning persistent identifiers and specifying a preferred citation format); article publishers should encourage authors to cite data formally in the reference list.
Data publishers should track and aggregate citations to their datasets to the extent feasible; at a minimum, they should pubilicize dowload counts, which are also valued by reserchers and easier to implement.

Data papers add value to datasets, partly through association with both through description and peer review.
Peer review is not an essential component of data publication, but it remains the gold standard of both trustworthiness and value in researchers' minds.
Data publishers 
What peer review of data means is still unclear, but two clear expectations are that true peers will supply domain expertise in their field and that evaluation of metadata will play a significant role.


% You may title this section "Methods" or "Models". 
% "Models" is not a valid title for PLoS ONE authors. However, PLoS ONE
% authors may use "Analysis" 
\section*{Methods}

\subsection*{Survey design and distribution}
All results were drawn from survey approved by the University of California, Berkeley Institutional Review Board. %is that right?
The survey instrument is available at XXX.
Respondents completed the sruvey anonymously.
Researcher affiliated with the University of California (UC) could supply an email address for follow-up assistance with data publication, but neither the fact of affiliation nor any UC-specific information was used in this analysis.

The survey contained 34-questions in 3 categories: demographics, data sharing interest and experience, and data publication perceptions.
Demographic questions collected information on respondent's country, type of institution, research role, and discipline.
Questions to assess respondent's background knowledge of data sharing and publication focused on knowlege of several relevant US governmental policies and an invitation to name data journals.
Data publication perceptions consisted of ``mark all that apply'' questions concerning definitions of data publication and peer review and Likert scale questions about the value of various possible features of a data publication.
The number of required questions was kept to a minimum.
Some questions were displayed dynamically based on previous answers. 
Consequently, $n$ varies considerably from question to question.

The survey was administered as a Google Form.
It was officially open from January 22nd to February 28 of 2014; two late responses received in March were included in the analysis.
Solicitations were distributed via social media (Twitter, Facebook, Google+), emails to listservs, and a blog post on Data Pub\cite{kratz_data_2014}.

\subsection*{Data processing and analysis}
Although the nature of the survey is benign and identification would be unlikely to negatively impact respondents, light anonymization was performed prior to analysis and release of the response data.
UC affiliation and answers to UC-specific questions were redacted.
Respondent locations were grouped into United States and ``other;'' this distinction was preserved only because background questions asked about US government policies.  
Sub-disciplines with fewer than three respondents were re-coded with the corresponding discipline.
Listed data journal names were standardized by hand, and free text answers to other questions were replaced with ``other.''

After anonymization, responses were filtered for analysis.
Because the goal of the survey was to learn about researchers specifically, we exempted from analysis anyone who self-identified as a librarian or information scientist.
To restrict the analysis to active researchers only, anyone who affirmed that they had not generated any data in the last five years was exempted; the 90 respondents did not answer this question were retained.
Finally, undergraduate students and anyone who did not possess at least a Bachelor's Degree were filtered out.
In total, 32 respondents were removed before analysis, some on the basis of multiple criteria.

Analysis was performed using IPython\cite{perez_ipython_2007}, Pandas\cite{mckinney_proc-scipy_2010}, and Numpy\cite{van_der_walt_numpy_2011}.
Graphs were prepared using matplotlib\cite{hunter_matplotlib_2007} and formatted with Adobe Illustrator.
For significance testing, Fisher's exact test was used for all 2x2 tables and contingency table chi-square for all larger tables.
A statistical significance cutoff of $\alpha=0.05$ was used, corrected for multiple hypothesis testing when appropriate.
Because few questions were required,  ``mark all that apply'' questions with no reply at all were considered to be skipped.
Questions that related to US policies were analyzed based on US respondents only; one questions about the NIH was analyzed based only on US biologists.

% Do NOT remove this, even if you are not including acknowledgments
\section*{Acknowledgments}


%\section*{References}
% The bibtex filename
\bibliography{SurveyPaper}

\section*{Figure Legends}
%\begin{figure}[!ht]
%\begin{center}
%%\includegraphics[width=4in]{figure_name.2.png}
%\end{center}
%\caption{
%{\bf Bold the first sentence.}  Rest of figure 2  caption.  Caption 
%should be left justified, as specified by the options to the caption 
%package.
%}
%\label{Figure_label}
%\end{figure}

\begin{figure}[!ht]
\begin{center}
%\includegraphics[width=4in]{figure_name.2.png}
\end{center}
\caption{
{\bf Policy knowledge.}  Rest of figure 2  caption.  Caption 
should be left justified, as specified by the options to the caption 
package.
}
\label{Figure_policy_knowledge}
\end{figure}


\section*{Tables}
%\begin{table}[!ht]
%\caption{
%\bf{Table title}}
%\begin{tabular}{|c|c|c|}
%table information
%\end{tabular}
%\begin{flushleft}Table caption
%\end{flushleft}
%\label{tab:label}
% \end{table}

\end{document}
