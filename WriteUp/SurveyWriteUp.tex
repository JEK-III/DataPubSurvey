% Template for PLoS
% Version 1.0 January 2009
%
% To compile to pdf, run:
% latex plos.template
% bibtex plos.template
% latex plos.template
% latex plos.template
% dvipdf plos.template

\documentclass[10pt]{article}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}
% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

\usepackage{color} 

% Use doublespacing - comment out for single spacing
%\usepackage{setspace} 
%\doublespacing


% Text layout
\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm 
\textheight 21cm

% Bold the 'Figure #' in the caption and separate it with a period
% Captions will be left justified
\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

% Use the PLoS provided bibtex style
\bibliographystyle{plos2009}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother


% Leave date blank
\date{}

\pagestyle{myheadings}
%% ** EDIT HERE **


%% ** EDIT HERE **
%% PLEASE INCLUDE ALL MACROS BELOW

%% END MACROS SECTION

\begin{document}

% Title must be 150 characters or less
\begin{flushleft}
{\Large
\textbf{Researcher perspectives on data publication and peer review}
}
% Insert Author names, affiliations and corresponding author email.
\\
John Ernest Kratz$^{1}$, 
Carly Strasser$^{1}$, 
\\
\bf{1} California Digital Library, University of California Office of the President, Oakland, CA, USA

$\ast$ E-mail: John.Kratz@ucop.edu
\end{flushleft}


% Please keep the abstract between 250 and 300 words
\section*{Abstract}

Data ``publication'' seeks to appropriate the prestige of authorship in the peer-reviewed literature as a reward for creators of a useful and well documented datasets. 
Data publication has been embraced as a means to promote data sharing and numerous initita, but the conversation in the scholarly communication community around what a data publication should be and how data peer review should work is ongoing. 
To contribute an important and neglected perspective, we surveyed ~250 researchers across the sciences and social sciences, asking what expectations``data publication'' raises and what features would be useful to evaluate the trustworthiness and impact of a data publication and the contribution of its creator(s).  
We found that expectations of data publication center on availability.
Most respondents did not expect published data to be peer-reviewed, but peer review was the significant source of trust and value for a published dataset.


\section*{Introduction}

In 1985--- almost 30 years ago--- Stephen Ceci surveyed 847 scientists and concluded, ``it is clear that scientists in all fields endorse the principle of data sharing as a desirable norm of science''\cite{ceci_scientists_1988}.
This endorsement has not weakened in the intervening decades; the importance of data sharing was affirmed in 2010 by more than 65\% of faculty at California Polytechnic State University (Cal Poly)\cite{scaramozzino_study_2012} and in 2013 by 94\% of surveyed researchers in the United Kingdom (UK)\cite{bobrow_establishing_2014}.
The respondents in 1985 advocated data sharing ``to allow replication and extension of one's own findings''\cite{ceci_scientists_2014}, and the twin 

Scientific reproducibility problem attracting attention in the scholarly\cite{ioannidis_why_2005, prinz_believe_2011, mobley_survey_2013} and mainstream(ADD CITATION) press could be addressed, in part, by opening up the data underlying research papers for scrutiny\cite{drew_lost_2013, collins_policy_2014}.
Beyond confirming existing analysis, data sharing reduces costs\cite{piwowar_data_2011} and enables questions to be addressed that wouldn't be possible otherwise(FIND A CITATION).

Despite enthusiasm for data sharing in principle, in practice, Ceci declares ``that something is amiss in the academy.''

Unfortunately, researchers still often fail to make their data available.
Vines et al. (2014) requested data from 516 articles published between 1991 and 2011 and got it less than half (47\%) of the time\cite{vines_availability_2014}
Alsheikh-Ali et al. (2011) found that 59\% of 351 papers they examined did not adhere to the data availability requirements of the journal where they were published.
Researchers agree that this is a problem.
In 1985, 59\% of scientists complained that their colleagues were disinclined to share data\cite{cedi_scientists_1988}
In 2010, 67\% of scientists responding to an international survey felt that ``[l]ack of access to data generated by other researchers or institutions is a major impediment to progress in science'' and 50\% felt that their own research had been impeded\cite{tenopir_data_2011}.
That same year, a focused survey of California Polytechnic State University faculty found that 65\% of respondents believed that data sharing is important, but less than half of those (48\%) frequently share their own data\cite{scaramozzino_study_2012}.

Why do even researchers who believe that data sharing is important not do it?
Previous surveys have uncovered are a number of reasons, including ethical concerns around human subject data, fear that the data will be used improperly, a desire to squeeze as many publications as possible out of the dataset, and fear that the data will be used without adequately crediting the creator.
Ethical concerns emerged in both UK reports and were the second most frequently cited (by 55\% of respondents) constraint on data sharing in the EAGDA report\cite{swann_share_2008, bobrow_establishing_2014}
The possibility of getting fewer publications out of a dataset came up in 60\% of a series of interviews of scientists in the US, more frequently than any other risk; fear of ``data theft'' was mentioned in 32\% of the interviews\cite{kim_institutional_2012}.
Above all else, the most consistent answer is that preparing and documenting data so that it is useful for others takes too much time.
In the UK, lack of time was cited by as a major constraint in a 2008 report by the Research Information Network (RIN)\cite{swan_share_2008} and 66\% of a respondents to a 2013 survey by the Expert Advisory Group on Data Access (EAGDA) felt constrained to some or great extent by time, more than any other factor\cite{bobrow_establishing_2014}.
Time was the most cited (by 44\%) unavoidable cost (as opposed to potential risk) to data sharing in Kim and Stanton's survey\cite{kim_institutional_2012}.
Lack of time was cited as a reason for not sharing data in the international survey of 2011 by 54\% of respondents, more again than any other factor\cite{tenopi_data_2011}.

Although the process of preparing data to be shared openly can undoubtedly be streamlined with proper planning, it will always take time and effort.
The real problem isn't that it takes time, it's that that effort is currently unrecognized.
In the EAGDA report, 55\% or respondents say lack of tangible recognition and rewards constrains data sharing, and at least 75\% of respondents felt that the UK Research Excellence Framework (REF) does not recognize data (to some or great extent) relative to publications, but that it should.
The need to compensate researchers for their time in the scholarly currency of prestige is a major driver of the movement toward data publication.
 
Data publication seeks to borrow terminology (``publication'', ``peer review'') from the existing publication system and in that way tie in to the existing academic reward system\cite{costello_motivating_2009, lawrence_data_2011}.
Data publications can be cited to accumulate credit to compensate researchers for the time spent preparing their data. 
The risk of missing out on publications is at least slightly offset if the researcher's contribution will be recognized.
Data publications will be entered into the scholarly record for preservation.

The model of data publication that is taking off right now is the data paper.
Data papers are descriptions of datasets including rationale and collections methods, without any analysis or conclusions\cite{newman_data_2009}.
Data papers are appears as a new article type in journals like \emph{F1000Research} and \emph{Internet Archaeology} and in dedicated journals such as \emph{Earth System Science Data}, \emph{Geoscience Data Journal}\cite{allan_geoscience_2014}, and Nature Publishing Group's \emph{Scientific Data}, which 
concisely sums up the idea as ``a publication venue that credits scientists who share and explain their data." \cite{_more_2014}
Data journals all offer peer review; reviewers are asked to evaluate the dataset, the description of the dataset, and whether the two form a complete and consistent package containing everything needed to use the data; Lawrence et al. (2011) describes a set of generic data peer review guidelines \cite{lawrence_data_2011}

However, data papers are far from the only type of data publication being implemented.
Data publishers include self-deposit repositories such as figshare, Dryad, and Zenodo with light metadata requirements and minimal validation.
Also, domain specific repositories like the National Aeronautics and Space Administration (NASA) Distributed Active Archive Centers (DAACs) that use complex internal and external review processes to ensure that data is high quality\cite{weaver_data_2012}.
Also, peer reviewed datasets with rich metadata that can be found in Open Context\cite{kansa_we_2014}.

This variety attest to the fact that data publication is a term that the scholarly communication community is still in the process of defining.
After noting a lack of both consensus and interest on the part of researchers, the RIN report of 2009 adopted a deliberately minimal definition, ``making datasets publicly available''\cite{swan_share_2008}.
Since then, more nuanced definitions have been put forth, such as a distinction between shared (available), published (also citable), and Published (also peer-reviewed) datasets\cite{callaghan_making_2012}.
We have argued that within the scholarly communication community, there is consensus that published data must be openly available and citable, but that what kind of validation (if any) is required to qualify is still an open question\cite{kratz_data_2014}.
In the conversation around these issues, it's easy to forget that data ``publication'' is a term that is not targeted at the scholarly communication community.
Ultimately, the point of data  point of data publication, as opposed to data sharing, data release, or any other term is that publication and peer review are already meaningful to researchers.

However, the RIN report of 2008 is the last time anyone asked them.
Whereas research data management and sharing have been the subject of several surveys, data publication has not.
What do data publication and peer review mean to researchers?
How well do current models meet their expectations?
What components of a data publication would they find useful for evaluating the data and its creators?
To address these questions, we conducted an online survey of active researchers in January and February of 2014.


% Results and Discussion can be combined.
\section*{Results}

\subsection*{Demographics}

We received 281 unique responses to the survey.
Because in most cases we did not contact potential respondents directly, but rather asked colleagues to forward the request on and posted on social media, we cannot estimate in any reasonable way how many researchers were ultimately contacted and therefore cannot calculate a response rate.
We analyzed the responses of 249 (81\%) of these who were deemed to be active researchers (summarized in Table 1).
The largest response was from biologists (37\%), followed by Archaeologists (13\%), Social scientists (13\%), and Environmental scientists (11\%).
Most (85\%) of the respondents were affiliated with academic institutions; 90\% of those are research-focused (rather than teaching).
Respondents covered the academic career spectrum: 41\% are principal investigators/lab heads, 24\% postdocs, and 16\% grad students.
We saw surprisingly few significant differences in response between disciplines or roles, so the results are presented in aggregate.
We received responses from 20 countries, but the overwhelming majority (79\%, 197) are from the United States (US).

\subsection*{Background knowledge}

We asked a number of questions to gauge respondents engagement and familiarity with issues around data sharing and publication.
Respondents were asked to rate their familiarity with three United States (US) federal government policies related to data sharing and availability.
Because the policies are specific to the US, we restricted our analysis to the respondents who work in the US.
The best known was the National Science Foundation (NSF)'s Data Management Plan requirement\cite{national_science_foundation_gpg_2011}.
Less than half of the respondents had heard of the United States Office of Science and Technology Policy (OSTP) Open Data Initiative\cite{obama_making_2013} at all; this may be because the directive has not had any concrete effect on researchers yet, even though it will eventually affect virtually everyone doing research in the US.
On the other hand, even though the National Institutes of Health (NIH) data sharing policy\cite{national_institutes_of_health_final_2003} was enacted 11 years ago, only 4 (5\%) biologists claimed to `know all the details,' while 18 (24\%) had `never heard of it.'

Because data papers are a rapidly proliferating form of data publication that has been gaining attention, we asked about data journals specifically.
We provided a free text box and asked respondents to list any data journals that they could name.
Only 16\% of respondents named a data journal; \emph{Ecological Archives} was the most frequently named, by 16 respondents. 
The second most common response was Nature Publishing Group (NPG)'s \emph{Scientific Data} (named by 14), even though it had not started publishing at the time of the survey.
\emph{Earth System Science Data (ESSD)} (7), \emph{Biodiversity Data Journal} (6) and \emph{Geoscience Data Journal} (5) followed.
A number of respondents listed non-journal data publishers: figshare (6), Dryad (3), and Zenodo(1).

\subsection*{Data sharing mechanisms}

One of the goals of this survey was to explore the distinction between data sharing and publication.
Unlike data publication, a relatively new and unfamiliar concept to researchers, most have experience with and opinions about data sharing.
Many respondents (56\%, 140) said that it is very important to share the data that underlies a study; we did not see any statistically meaningful difference between disciplines ($\chi^{2}= 39.1, p= 0.18$). %Check that these values makes sense!
A majority (68\%, 168) have shared data in the past.
Of those, 58\% (98) saw their data reused by someone, and 62\% (61) of those reuses led to publication.
Slightly fewer, 61\% (151) had reused someone else's data; 69\% (104) of these led to a publication.

Because the channel of sharing is a crucial feature of data publication <CITE>, we asked about sharing channels (Figure 2).
We asked how respondents shared data, how others got their data, and how they got others data.
We provided four answers that correspond to the four methods for external data sharing that emerged from interviews by Kim\cite{kim_institutional_2012}: email/direct contact, database or repository, and journal website.
In all cases, email/direct contact was the most frequently reported channel for sharing; 87\% (146) shared data that way, 82\% (82) reported that others got their data that way, and 57\% (86) reused data that they got that way.
As a fifth alternative, 5\% (8) respondents wrote-in that they had extracted data from the text, tables, or figures of a published paper.

\subsection*{Data sharing credit}

A primary goal of data publication is to enable researchers who do the work needed to make data reusable to get credit for their work.
We wanted to assess what the researchers taking our survey thought was appropriate credit for the creators of a dataset.
We asked how a researcher who shares data should be credited.
Formal citation in the reference list was the most common answer, given by 83\% (126)
Acknowledgement also ranked highly at 62\% (93)
The most common actual practice observed by examining published articles, informal reference in the body of the paper\cite{sieber_not_1995, mooney_anatomy_2012}, ranked lowest at 16\% (24).
Respondents who used the provided free-text field overwhelmingly wrote some variant on ``it depends.''
Two factors were cited: the publication status of the dataset (e.g. ``depends on whether the data is already published'') and the role of the data in the paper (e.g. ``authorship if data is [the] primary source of analysis, otherwise acknowledgment'').
Because previous studies identified substantial differences in culture between disciplines\cite{harley_assessing_2010, swan_share_2008}, we tested for significant differences between disciplines (omitting Mathematics because the \emph{n} was too low for chi-square), but did not find any ($\chi^{2}= 15.6, p= 0.9$).

We also asked the subset who had published with someone else's data how they actually credited the creators.
The answers to this question corresponded well with the prior theoretical question.
Formal citation was the most frequently reported, by 63\% (81), followed by acknowledgment (LOOK UP).
There was a statistically significant ($\chi^{2}= 15.8, p= 0.0013$) difference between what respondents said should be done and what they actually did.
The likely source of this distinction is that no one admitted to citing data informally in the body of the text; otherwise responses were roughly similar.


Fear of `data vultures' appropriating and profiting from one's data without adequately acknowledging the source is a significant concern for researchers \cite{kim_insitutional_2012}, but we wanted to know how significant the problem really is.
To address this question, we asked respondents whose data had been reused in a publication how they felt about the amount of credit given.
Most of the 86 who answered the question, 63\% (54) felt that they had been credited appropriately, and a combined 78\% (67) thought they received appropriate or excessive credit.
This left 22\% (13) who felt that they received insufficient credit; only 2 (2\%) felt that the credit was `very insufficient.'
We considered the possibility that differences in satisfaction were driven by different attitudes toward what constitutes appropriate credit.  
To that end, we collapsed responses into three categories (insufficient, appropriate, and excessive), and tested for independence with each of the four specified answers in data sharing credit.
The strongest relationship, between less satisfaction and ``authorship on paper,'' was still far from statistically significant ($\chi^{2}= 3.26, p= 0.20$, corrected $\$alpha= 0.0125$).

\subsection*{Definitions}

Because the application of ``publication'' and ``peer review'' to data is largely based on the familiarity of these ideas to researchers, we wanted to know what ``data publication'' and ``peer review of data'' would actually mean to researchers.
We decomposed the prevalent models of data publication into a set of separate features and asked which would be expected to distinguish a ``published'' dataset from a ``shared'' one (Figure XXXA).
Not surprisingly, the most common expectations relate to access: 68\% (166) expect a published dataset to be openly available and 54\% (133) expect it to have been deposited in a repository.
More researchers expected a published dataset to accompanied by a traditional publication (43\%, 105) than a data paper (22\%, 55).
Perhaps surprisingly, only a 29\% (70) minority expect published data to have been peer reviewed.

To find out what researchers expected from data peer review, we asked what they would expect peer review of data to entail and provided a list of considerations that various organizations take into account when evaluating/validating datasets (Figure XXXB).
The most common responses were ``collection and processing methods were evaluated'' (90\%, 220) and ``descriptive text is thorough enough to use or replicate the dataset'' (80\%, 196).
The high rank of the latter suggests an awareness of the importance of metadata.
Fitting with current practice, there was little (22\%, 53) expectation that peer review would consider the novelty or potential impact of the dataset.
An expectation that data publication includes peer review correlated positively (by Fisher exact test) with thorough descriptive text ($OR= 5.67, p= 0.00029$), plausibility considered ($OR= 2.29, p= 0.0087$), and novelty/impact considered ($OR= 2.42, p= 0.0092$), although only the first edges over an $\alpha= 0.05$ significance threshold after applying the Bonferroni correction for multiple hypothesis testing.

We were interested in whether selections of items in each definition represented coherent ideas about publication or peer review, so we tested agains the null hypothesis that each item was selected (or not) independently of every other.
Not surprisingly, this hypothesis was rejected resoundingly for both data publication ($\chi^{2}$= 290.0, $p= 6.9\times10^{-58}$) and peer review ($\chi^{2}= 343.0, p= 5.7\times10^{-72}$) by chi square tests.
To identify specific correlations, we performed pairwise Fisher exact tests between each feature (Figure XXX).
We found a dense set of statistically significant (at the $\alpha= 0.05$ level, corrected to 0.0018) associates among features related to access and preservation.
For instance, ``deposited in a repository'' was linked to ``openly available'' ($OR=9.55, p= 7.6\times10^{-14}$), unique ID ($OR= 3.33, p= 1.33\times10^{-5}$), formal ($OR= 4.49, p= 3.94\times10^{-6}$) and rich ($OR= 3.83, p=1.14\times10^{-6}$) metadata.
We were unsurprised to find linkages between formal and rich metadata ($OR= 12.5, p= 2.7\times10^{-14}$) and between assignment of a unique ID and formal metadata ($OR= 7.92, p= 5.05\times10^{-11}$).
Data papers were linked to both rich ($OR=3.64, p=4.22\times10^{-5}$) and formal ($OR=4.30, p=1.32\times10^{-5}$) metadata.
Data papers were also the only feature associated with peer review ($OR=3.00, p=0.0011$).
``Basis of a traditional paper'' had no significant associations; the closest was with data paper ($OR=1.93, p=0.023$).

We also found multiple significant relationships among possible considerations for peer review, with a corrected cutoff of $\alpha=0.0033$.
Three strong correlations formed a triangle from ``methods are appropriate'' to ``Metadata standardized'' ($OR=7.91, p=0.00081$) to ``Technical details check out'' ($OR=4.56, p=3.4\times10^{-6}$) to ``methods are appropriate'' ($OR=5.9, p=7.9\times10^{-5}$).
``Data is plausible'' correlated with other factors that require domain expertise: ``methods are appropriate'' ($OR=4.51, p=0.0014$), ``metadata sufficient for replication'' ($OR=4.87, p=2.5\time10^{-6}$), and ``novelty/impact'' ($OR=5.50, p=1.1\times10^{-5}$).
Interestingly, this was the only association for ``novelty/impact''

\subsection*{Values}

A significant goal of data publication is to facilitate reuse of data.
Consequently, we wanted to know what features of a data publication would be useful to researchers in evaulating a dataset's quality.
To address this question, we asked respondents to rate how much confidence each of four possible assurances of data quality inspires.
All of the features inspired at least some confidence in a large majority of researchers (from 89\% to 98\%).
Peer review was easily the most important factor; 72\% (175) said it conferred complete or high confidence, while only 2\% (4) expressed little or no confidence.
Having been the subject of a traditional paper was second with 56\% (137) complete or high confidence.
Reuse by a third party came in 3rd, with 43\% (106) and description by a data paper was the least convincing at 37\% (89).
The largest fraction of little or no confidence was reuse, at 11\% (25).
%could try to relate peer review confidence to definition, but the results aren't that clear / don't make that much sense

A second significant goal of data publication is to facilitate researcher's getting credit for their datasets.
To that end, we wanted to know what metrics researchers would respect in evaluating the impact of a dataset.
Not surprisingly, citation count was considered to be the most useful metric, with 49\% (119) finding it highly or extremely useful. 
However, downloads also did surprisingly well, at 32\% (77) highly or extremely useful.
If the threshold is dropped to at least somewhat useful, the distinction shrinks further, to 82\% (201) for citations and 73\% (179) for downloads.
Search rank and altmetrics were considered substantially less useful, at 42\% (102) at least somewhat useful for search rank and 37\% (91) for altmetrics, in both cases a minority.

Another way we approached the question of how to attach credit to data publications is to ask how much weight each of a number of generic data publication models would be given as an item on a researcher's CV.
We asked about four models, with or without the imprimatur of peer review or the inclusion of a data paper. 
As a baseline, we also asked about traditional papers.
Not surprisingly, they are highly valued; 60\% (145) give one a great deal of weight and another 36\% (87) give it significant weight.
The most highly weighted data publication, a peer-reviewed data paper, was much less highly valued at only 10\% (23) a great deal of weight, although another 46\% (109) gave it significant weight.
Peer review appeared to be more important than having a data paper. 
Compared to that 10\%, a peer-reviewed dataset with no paper only dropped to 5\% (12), while an un-peer-reviewed data paper dropped to 1\% (2).
A significant proportion of respondents, 27\% (65) would award an un-peer reviewed dataset no weight at all.

For this question, we were particularly interested in the answers of 26\% (59) respondents who had served on a tenure and promotions committee, so we compared their responses to those who had not served on such a committee by chi square.
Values ranged from $\chi^{2}= 7.427$ ($p= 0.12$) for peer-reviewed datasets (valued slightly less by experienced respondents) to $\chi^{2}= 2.097$ ($p= 0.35$) for traditional papers (valued slightly more by experienced respondents); for 5 comparisons, $\alpha=0.01$, and none can be considered statistically significant


\section*{Discussion}

\subsection*{Demographics}

Although this survey was international, a substantial majority of our respondents are affiliated with United States institutions, due in part to significant response from within the University of California system.
In this respect, our respondents (84\% from North America) are similar to those of Tenopir (2012), who were 73\% from North America.
Several of the more substantial investigations into researcher attitudes toward data sharing, such as those carried out by the Wellcome Trust\cite{bobrow_establishing_2014} and Research Information Network\cite{swan_share_2008} were carried out in the United Kingdom, where the primacy of a single assessment tool, the Research Excellence Framework (REF) is likely to create a significantly different environment.
At least 75\% of respondents in the Wellcome Trust report felt that the REF should recognize data to some or great extent relative to publications, but that it should.

The bulk of our responses (85\%) came from academic institutions, which is similar to Tenopir's 81\%.  
Ceci's initial survey was academic\cite{ceci_scientists_1988}; Scaramozzino's was conducted entirely at California Polytechnic State University, which is teaching-oriented\cite{scaramozzino_study_2012}. 

Our survey covered active researchers with good coverage of all of the major roles in academic research.
Tenopir has 47\% professors, 13.5\% grad students, we have 41\% principle investigator/lab heads and 16\% grad students.
Most other surveys were restricted to PIs.
The most recent UK survey heard from 69\% PIs\cite{bobrow_establishing_2014}.

Our Largest response was from biologists: (37\%), followed by Archaeologists (13\%), Social scientists (13\%), and Environmental scientists (11\%). 
Tenopir is most heavily environmental sciences and ecology (36\%), followed by social science (16\%) and biology (14\%). 
We're heavy in biology (37\%), but with a significant representation from Earth and environmental science (16\%) and social science (13\%). 
Scaramozzino heard from 18\% biologists, with the highest from physicists and mathematicians.
Swann investigated a restricted set of eight research areas.
The Wellcome survey is also heavy in social science (31.4\%), Epidemiology, (26.8\%), Genetics/genomics (20\%)

Because participation in the survey was entirely voluntary and uncontrolled, responses are almost certainly biased toward researchers with an interest in data sharing and publication related issues.
However, the high proportion of respondents who did not name any data journals (84\%) and the low awareness of US federal policies (35\% of US respondents had never heard of the NSF data management plan requirement and 62\% had never heard of the OSTP Open Data Initiative) suggests that our sample is not extremely biased.
To provide context, 40\% of respondents to the Wellcome Trust survey were unaware of data journals.


\subsection*{Data publication}

We found that there  is no single clear meaning of the term ``data publication'' to researchers.
The most frequently named attribute, open availability, was only chosen by ~2/3 of respondents.
It may be that little has changed since the RIN report concluded that ```publishing' datasets means different things to different researchers''\cite{swan_share_2008}.
One respondent to our survey simply wrote in ``terms are confusing.''
However, the emergence of systematic relationships between some of the features presents demonstrates that the responses were not utterly confused.
We observed two, arguably three, independent conceptions of data publication.
The first centers on preservation and access; the two most popular items, availability and deposition in a repository, were tightly correlated.
Other related features, assignment of a unique ID, and the two items relating to metadata are all correlated with deposition in a repository, and many links exist between those five items.
This idea maps well onto virtually all present data publication implementations.

The second is a traditional conception that data that has been ``published'' is really just data that has been used in a traditional journal article. 
``Basis of a research paper'' was choses by a sizable proportion (43\%) of respondents.
Interestingly, rather than competing significantly, these two ideas were completely independent.
This idea doesn't really match the conversation in the scholarly community; while a great deal of published data (in, e.g. Dryad) has been used by a traditional article, data that had been used but not made available has not been published.

The third conception is that a published dataset is one that has been described by a data paper.
Data papers are linked to peer review and both kinds of metadata, but not significantly associated with features that have to do with the disposition of the data, such as deposited in a repository or openly available, even though virtually all data paper publishers require both peer review and repository deposition.
Data papers are still not widely familiar to researchers; ``described in a data paper'' was the least popular (22\%) feature selected, and most (84\%) respondents did not name any data journals.
Data papers also confer less trust in a dataset than any other feature we asked about, but only by a small margin; 36\% of respondents derive high or complete confidence from a data paper, compared to 44\% from successful reuse.
Not surprisingly, data papers are much less highly valued than traditional research papers; 60\% of respondents give a traditional paper a great deal of weight, but only 10\% value a data paper that highly.
It may be that data papers will become more valued as awareness spreads; when asked to name data journals, one respondent wrote in ``I've never heard of this, but it sounds fantastic.''
On the other hand it may be that a data paper \emph{should} be valued less than a traditional paper and 55\% would give a data paper at least significant value, so that might be appropriate.
It is clear that a data paper adds value to a dataset, however, when the two are uncoupled, peer review is the bigger factor.
 
\subsection*{Data validation}

The type and degree of quality control is perhaps the most variable aspect of current data publication implementations. 
Commenting on the variable meanings of data publication to researchers, the RIN report of 2008 remarked, ``[t]here is, for some, also an implication that the information has been through a quality control process.''\cite{swan_share_2008}
Substantial trust is attached to the traditional scholarly publication process; researchers expressed a great deal of trust in datasets that had formed the basis of a traditional paper.

Peer review was both the most significant factor in boosting the trustworthiness of dataset, and in boosting the value of dataset listed on a CV.
This may, in part, reflect the high proportion of biologists in our survey; biologists value peer review especially highly\cite{harley_assessing_2010}
Although peer review was highly valued, it was not expected by most researchers; 71\% would \textbf{not} expect a published dataset to have been peer-reviewed.
Expectations for peer review seem to be more consistent than for publication, despite one respondents remark that ``I have never heard this term applied to a dataset and I don't know what it means.''
Whereas the top item in the publication definition question was only chosen by ~2/3 of respondents, 90\% of respondents agreed that they would expect peer review to include evaluation of collection and processing methods.
In fact, fully half of the items were chosen by more respondents than the top data publication item.
Naturally, a majority of respondents expect assessments that require domain expertise.
``Data is plausible'' was linked with three other features that require domain knowledge (methods evaluated, sufficient metadata to replicate, and potential impact/novelty).
In general, respondents did not expect peer review to take novelty/impact into account (only 22\%), which is in line with most data peer review guidelines.
We conclude that data publication initiatives that do not incorporate peer review are unlikely to confuse anyone, but that peer review is likely to be critical for data publications to attain a status comparable to that of journal articles.

Based on our personal experiences, we were surprised that ``successful reuse'' did not inspire more trust in respondents.
The idea that ``data use in its own right provides a form of review''\cite{parsons_data_2010} is frequently put forward in discussions of data publication, but successful reuse rated below both peer review and ``basis of a traditional paper'' as a means of establishing trust in a dataset.
It's worth nothing that ``basis of a traditional paper'' is, itself, evidence of use.
Interestingly, citations, a direct consequence of successful reuse, were the most useful metric for assessing data value/impact.
The seeming disconnect could be the result of different ideas about what constitutes successful reuse vs. where a citation comes from, or the difference between trustworthiness and impact.
Combined, the value of reuse/citations for increasing trust in dataset reusers and establishing the value of the creators work makes reuse as review highly worthwhile, but it does appear that it can't substitute for peer review.

\subsection*{Credit for data}

The scholarly communication community agrees that data should be cited formally in the reference list,\cite{force11_data_citation_synthesis_group_joint_2014} but past studies found that this is rarely actually done.\cite{sieber_not_1995, mooney_citing_2011, mooney_anatomy_2012}
In a 1995 survey of 198 papers that used published datasets, 18.7\% cited the dataset with at least its title in the reference list\cite{sieber_not_1995}, and a followup 17 years later found, with 16.9\% meeting this low standard, that practice had not improved.\cite{mooney_anatomy_2012}
The most common taken approach is informal citation in the methods or results section of the paper; 30.8\% of papers in 1995 and 69.2\% in 2012 included at least the title in their text.
Despite this dismal state of affairs, in this survey, most respondents affirmed that formal citation is the correct approach.
Formal citation was the most common response to how a dataset creator should be credited (83\%) and to how the respondent actually credited data creators (63\%).
Furthermore, no one admitted to informally citing data in the text of a paper.
It may be that our respondents were particularly conscientious, that significant forms of credit are easier to remember, or that researchers are operating on somewhat different definitions of what constitutes a dataset or reuse than the authors of the surveys based on examining papers.
It is entirely possible, for instance, that a biologist who uses a sequence from GenBank would not think of such a thing as a dataset or of their activity as reuse.
The Wellcome Trust report found that 71\% of their respondents already tracked use of their datasets ``through details of publications generated using the data.''\cite{bobrow_establishing_2014} 
This suggests that efforts to advance data citation should focus on implementation, rather than making the case in the first place.

While citations were considered the most reliable metric of a dataset's value, download counts were also valued quite highly.
Furthermore, 43\% of respondents to the Wellcome Trust survey track downloads of their datasets\cite{bobrow_establishing_2014}
It can be considerably easier for a repository to count downloads than to harvest citations, so download counts represent ``low hanging fruit'' in terms of data metrics.
On the other hand, appreciation of altmetrics was quite low; few gave them much weight, but a majority gave them a little.
Altmetrics are still very much in development for papers, so it is perhaps not surprising that researchers don't know what to make of them with data.
There is certainly no harm in providing altmetrics for datasets, but in the short term, they are unlikely to add much credibility.

Fear of data theft without credit was the biggest risk preying on researcher's minds (32\% of their minds, to be more precise) in Kim and Stanton's survey of 2012.\cite{kim_institutional_2012}
To try to get a sense of how well-founded these fears are, we asked researchers how they felt about the credit they got the last time someone else published using their data.
While the majority felt that they got the right amount of credit, the fraction that felt shortchanged (22\%) is too large to ignore, and we have to conclude that this is a real problem.
We make no judgment here about whether the expectations of the unsatisfied researchers are reasonable or not, but we did not find any significant differences between satisfied and unsatisfied respondents in the view of how a dataset should be credited.
As time goes on, discipline-specific data-use etiquette may solidify, reducing the problem.
An expectation of authorship may always be a problem.
However, enabling and encouraging data citation can only help.  

\subsection*{Data publication redux}

The results of this survey provide some practical guidance for how data publication implementations can meet researcher expectations and provide the most value possible.
Above all else, researchers expect published data to be accessible, and, secondarily to be deposited in a repository; this fits with current practice and with the most fundamental idea of publication.
Data publications must enable formal citation; researchers felt that this was the most appropriate way to credit a dataset creator in theory and practice.
Tracking citations to a dataset is valuable for evaluating the quality and impact of a dataset; however, download counts are also valuable for assessing impact and may be easier to implement.
Data publications that include a data paper are more valued as CV items and better trusted; this may be, at least in part, because they are associated with peer review.
Peer review is not an essential component of data publication in researchers minds, but it continues to be the gold standard in researchers minds for both trustworthiness and value, and data publishers would be well advised to provide it if possible.
What peer review of data means is still unclear, but two clear expectations are that true peers will supply domain expertise in their field and that evaluation of metadata will play a significant role.


% You may title this section "Methods" or "Models". 
% "Models" is not a valid title for PLoS ONE authors. However, PLoS ONE
% authors may use "Analysis" 
\section*{Methods}
\subsection*{Survey design and distribution}
All results were drawn from a 34-question survey officially open online from January 22nd to February 28 of 2014; two late responses received in March were included in the analysis.
The University of California, Berkeley Institutional Review Board approved the survey.
The survey could be completed anonymously.
Respondents affiliated with the University of California had an opportunity to supply an email address for help with data publication, but that information was not used in any way for the purposes of this article.

The survey asked questions in 3 categories: demographics, data sharing experience, and data publication perceptions.
Data publication perceptions consisted of ``mark all that apply'' questions about the definition of data publication and peer review and Likert scale questions about the value of various possible features of a data publication.

The survey was designed with a minimum of required questions.
Some pages were displayed dynamically based on answers to certain questions; therefore, n varies considerably.
The survey was administered as a Google Form.
Solicitations were distributed via social media (Twitter, Facebook, Google+), emails to listservs, and a blog post on Data Pub\cite{kratz_data_2014}.

\subsection*{Data processing and analysis}
Although opinions are unlikely to be controversial, light anonymization was performed prior to analysis.
University of California (UC) affiliation and questions with specific application to UC researchers were redacted.
Respondent locations were grouped into United States and ``Other''; this distinction was preserved because some questions asked about US government policies.  
Sub-disciplines with fewer than three respondents were merged into the corresponding discipline.
Listed data journals were standardized by hand.
Free text answers were replaced with ``Other.''

After anonymization, responses were filtered for analysis.
Because the goal of the survey was to learn about researchers, we exempted from analysis anyone who self-identified as a librarian or information scientist.
To restrict the analysis to active researchers, we filtered anyone who affirmed that they had not generated any data in the last five years; the 90 respondents who did not answer this question were retained.
Finally, we exempted anyone who did not possess at least a Bachelor's Degree.
In total, 32 respondents were exempted from analysis, some on the basis of multiple criteria.

Analysis was performed using IPython\cite{perez_ipython_2007}, Pandas\cite{mckinney-proc-scipy-2010}, and Numpy\cite{van_der_walt_numpy_2011}.
Graphs were prepared using Python and formatted with Adobe Illustrator.
For significance testing, the Fisher exact test was used for 2x2 tables and contingency table chi-square for all larger tables.
A significance cutoff of $\alpha=0.05$ was used, corrected for multiple hypothesis testing when appropriate.
Because few questions were required, questions with no reply at all were considered to be skipped rather than an intentional ``none of these''.


% Do NOT remove this, even if you are not including acknowledgments
\section*{Acknowledgments}


%\section*{References}
% The bibtex filename
\bibliography{SurveyPaper}

\section*{Figure Legends}
%\begin{figure}[!ht]
%\begin{center}
%%\includegraphics[width=4in]{figure_name.2.png}
%\end{center}
%\caption{
%{\bf Bold the first sentence.}  Rest of figure 2  caption.  Caption 
%should be left justified, as specified by the options to the caption 
%package.
%}
%\label{Figure_label}
%\end{figure}


\section*{Tables}
%\begin{table}[!ht]
%\caption{
%\bf{Table title}}
%\begin{tabular}{|c|c|c|}
%table information
%\end{tabular}
%\begin{flushleft}Table caption
%\end{flushleft}
%\label{tab:label}
% \end{table}

\end{document}
