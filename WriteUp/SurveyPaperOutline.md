#Roadmap
##Abstract
* Data "publication" attempts to appropriate for data the prestige of publication in the scholarly literature. 
* While the scholarly communication community substantially endorses the idea, it hasn't fully resolved what a data publication should look like, or how data peer review should work.  
* To contribute an important and neglected perspective on these issues, we surveyed ~250 researchers across the sciences and social sciences, asking what expectations "data publication" raises and what features would be useful to evaluate the trustworthiness and impact of a data publication and the contribution of its creator(s).  
* We found that the most widely-held perceptions of data publication as opposed to sharing related to access and preservation.
* Although most respondents did not published data to have been peer-reviewed, peer review was regarded as highly valuable; peer review of a dataset inspired more confidencein a dataset than publication with a traditional paper and and peer reviewed datasets would be considered substantially more heavily when presented on a CV.


##Introduction
1. Motivate data publication
2. Open definitions / questions
3. Discuss sharing surveys

###access to research data
####Theory
* reproducibility
* *reproducibility "crisis" / problem \cite{ioannidis_why_2005, prinz_believe_2011, mobley_survey_2013}
* *data publication could be a solution to the crisis \cite{drew_lost_2013, collins_policy_2014}
* reuse
* *$ spend archiving data contributes to more papers than $ spent collecting new data \cite{piwowar_data_2011}
* *reuse of data can make stuff cheaper and enable question to be answered that couldn't otherwise


####Practice
* As early as a 1985 survey, "it was clear that scientists in all fields endorse the principle of data sharing as a desirable norm of science."\cite{ceci_scientists_1988}.
* Since then, lots of surveys of researcher practices and perceptions around data sharing\cite{swan_share_2008, harley_assessing_2010, tenopir_data_2011, kim_institutional_2012, scaramozzino_study_2012}
* In fact, researchers often fail to make their data available.
* Recent investigations found that 59% of papers did not adhere to the data availability policy of the journal \cite{alsheikh-ali_public_2011}
* Vines (2014) could only get the data from 47% of papers published between 1991 and 2011. \cite{vines_availability_2014}
* While 87% of scientists said that they shared their data on request, only 41% said that their colleagues did likewise. \cite{ceci_scientists_1988}.
* Scientists agree that this is a problem: 67% of scientists in a 2011 survey agree that "Lack of access to data generated by other researchers or institutions is a major impediment to progress in science" and 50% agreed that "Lack of access to data generated by other researchers or institutions has restricted my ability to answer scientific questions." \cite{tenopir_data_2011}
* While more that 65% of respondents to Scarmozzino et al. (2011) beileve that open data sharing is important, less than half (48%) of those say that they frequently share their own data. \cite{scaramozzino_study_2012}
* 76% of Finnish Social Scientists were positive toward open access to their own data. \cite{kuula_open_2008} 
* 60\% say making research data available is a key priority, and another 34.3\% say its "Quite important, but not a major priority"\cite{bobrow_establishing_2014}

* time is the most frequently named cost \cite{tenopir_data_2011, kim_institutional_2012, bobrow_establishing_2014} to data sharing.
* *66% constrained to some or great extent in Boborow (the most frequently cited constraint)
* *54% cited as a reason in Tenopir
* *44% mentioned time to organize data in Kim (the most frequently cited cost).
* *Cited as a major constraint on data publication in swan (2008)\cite{swan_share_2008}

* Other major constraints
* *fears associated with misuse: others might use data without crediting you, might misuse data, might do something that you could/were planning to have done with it.\cite{swan_share_2008}
* Ethical / legal issues around human subject data. \cite{swan_share_2008}
* *second biggest constraint (55% to some or great extent) in EAGDA report \cite{bobrow_establishing_2014}



###data publication vs. sharing
 
* Data sharing is good, buy we need/ want to move from 'sharing' to 'publishing' \cite{costello_motivating_2009, lawrence_data_2011}
* the idea in data publication is to borrow terminology ('publication', 'peer review') from the existing publication system and in that way tie in to the existing academic reward system. 
* Data publications can be cited to accumulate credit
* * compensates you for the cost of your time
* * if someone scoops you, at least you get *something* for your effort
* * "Scientific Data provides a publication venue that credits scientists who share and explain their data." \cite{_more_2014}
* data publications will be entered into the scholarly record for preservation.
* 55% say lack or tangible rewards and recognition constrains data sharing to some or great extent. Report concludes "Data sharing can be most effectively rewarded by increasing recognition of data sharers"\cite{boborow_establishing_2014}


* *simple: "sensible to adopt a pragmatic definition of "publication of datasets" as "making datasets publicly available"\cite{swan_share_2008}
* *nuanced: share vs. publish vs. Publish\cite{callaghan_making_2012}
* *ours: available, citable, maybe validated \cite{kratz_data_2014}

* Lots of data publication initiatives are steaming ahead
* in particular, lots of new venues for data papers
* however, it's still not clear what 'data publicaiton' means, and 'data peer review' even less so


####Data papers
* Data papers: description w/o analysis \cite{newman_data_2009}
* Earth System Science Data (2009), Geoscience Data Journal \cite{allan_geoscience_2014}, Nature Scientific Data (2014) \cite{_more_2014}
* provide a bona-fide paper, albeit not quite the usual, that can be cited and listed on a cv
* Peer review of data and paper wrapped together.
 
####Other stuff
* bare bones: figshare (http://figshare.com/), Dryad (http://www.datadryad.org/)
* complex: National Aeronautics and Space Administration (NASA) Distributed Active Archive Centers (DAACs) \cite{weaver_data_2012}
* *peer review by product team and user review board
* bespoke: Open Context \cite{kansa_we_2014}
* *editorial and external peer review
* Lawrence criteria\cite{lawrence_citation_2011}

###This survey
* there's quite a lot of stuff out there called data publications
* if we want to use those words, we'd better figure out what they mean to the real audience: researchers

* whereas data sharing has been the subject of many surveys/reports, data publication has not.
* *Swann and Brown (2009) talk about data publication, but they define it simply as “making datasets publicly available.”

* therefore we decided to survey researchers directly about:
1. what they think 'data publication' and 'data peer review' mean
2. what potential components of a data publication would they find useful for evaluating the data and its creators


* How well do current implementations of data publication meet the expectations of reasearchers?
* What features would be useful to a researchers?

##Methods

###Ethics

* UC Human Subjects IRB approved this survey
* UC respondents had the option of identifying themselves, but that information is omitted here
* Officially open January 22nd - February 28, 2014.  Got 2 responses in March, last one on the 25th
* Distributed by email, social media (twitter, Facebook)
* Open to anyone

###Distribution

* Cover letter with link to survey sent
* *UC campus libraries
* *Librarian Listservs
* Social media
* *twitter
* *Facebook
* Blog post on DataPub

###Survey
* Few required questions
* Some questions presented based on answers to others, therefore n varies considerably from question to question
* Demographics (minimal)
* Data sharing experiences / attitudes / knowledge
* Data publication perceptions
* *expectations from the terms 'data publication' and 'data peer review'
* *value of various possible DP features in various contexts

###Anonymization

* Location converted to US and 'Other'
* UC & campus affiliation redacted
* * knowledge of UC OA mandate redacted
* free text answers standardized
* * data journals converted to standardized list by hand
* * all text boxes associated with 'Other' buttons recoded as 'other', except for a limited number of cases in 'role', where judgement was used to assign the respondent to one of the standard responses
* Sub-disciplines with < 3 respondents were merged into the corresponding discipline

###Filtering for analysis

* Removed anyone who self-identified as a librarian
* Removed information scientists
* Removed anyone who said they hadn't generated any data in the last 5 years
* *90 respondents didn't answer.  too many to discard, so we kept them.
* Removed anyone whose highest degree was from high school
* We filtered out:
* *16 librarians
* *13 information scientists (6 of whom are also librarians)
* *3 without college degrees
* *11 who hadn't generated any data (2 librarians, 1 info scientist)
* *sometimes the same person was filtered by multiple criteria

##Analysis

* Analysis was performed using Python, IPython\cite{perez_ipython_2007}, Pandas\cite{mckinney-proc-scipy-2010}, Numpy\cite{van_der_walt_numpy_2011}.
* Fisher exact test was used for 2x2 tables; all pairwise comparisons.
* Contingency table chi-square was used for all larger tables.
* Consider questions where no answer was marked as skipped, rather than an intentional 'none of these' answer.
* Graphs were prepared using Python, then reformatted with Adobe Illustrator.



##Results

###Demographics
* In demographics table
* 281 unique responses   
* 249 (89%) passed filters for analysis
* Largest response from biologists: (37%), followed by Archaeologists (13%), Social scientists (13%), and Environmental scientists (11%)
* Responses from across the career spectrum: 41% principle investigator/lab heads, 24% postdocs, and 16% grad students.
* Overwhelmingly, biggest response (85%) from academic institutions, 90% of those are research-focused.
* We recieved responses from 20 countries, but the overwhelming majority, 79% (221) (unfiltered) were from the United States (US)
* we saw fewer differences between groups than expected, so we won't be presenting any

###Background knowledge
* attempted to gauge familiarity with issues around data publication/sharing/access


####Policies
* In policy awareness figure
* we asked respondents to rate their familiarity with 3 US federal policies related to data sharing/availability
* b/c policies are specific to US, we only report the results from the 197 (79%) of respondents who are in the US
* United States Office of Science and Technology Policy (OSTP) Open Data Initiative \cite{obama_making_2013}
* *not yet relevant to scientists, but will eventually impact everyone
* *less than half 75 (38%) have heard of it at all
* NSF DMP requirements \cite{national_science_foundation_gpg_2011}
* *the most well-known of the bunch
* *32 (16%) knew all the details
* NIH data sharing policy \cite{national_institutes_of_health_final_2003}
* *only looked at biologists (n=71)
* *only 4 (5%) claimed to "know all the details", much fewer than the 18 (24%) who had never heard of it
* *this even though the policy was enacted 11 years ago


####Data journals
* No figure
* most frequently named: ecological archive (16), Nature's Scientific Data (14)
* then, ESSD (7), Biodiversity Data Journal (6), Geoscience Data Journal (5)
* 29% (39) of respondents who answered the question in some way named at least one data journal
* 16% (39) of all respondents named a data journal
* a number of respondents listed or asked about repositories: figshare (6), dryad (3), zenodo (1)

####Sharing importance
* a majority (56% / 140) of respondents said that it is very important to share the data that underlies a study
* there was no statistically significant difference between disciplines (chi2 = 39.1, p=0.18)


###Sharing background
* In Sharing Experience figure
* asked about prior experience with sharing own data and using others
* some background attitudes about sharing
* 68% (168) shared data, 58% (98) of those had their data reused, and 62% (61) of those reuses led to a publication.
* 61% (151) reused others data, 69% (104) of those led to a publication. 

###Channels
* we asked three questions about mechanisms for sharing
* *Have you shared data in any of the following ways?
* *[If] your data [has] been re-used by anyone outside your research group / collaborators, how did they get your data?
* *[If] you ever re-used data from another research group (not as part of an existing collaboration), how did you get the data?
* respondents were instructed to check all answers that applied
* provided answers were equivalent to the 4 methods for external data sharing found by Kim (2012) \cite{kim_institutional_2012} in interviews with faculty:
* *Email / direct contact
* *database or repository
* *journal website (as supplemental material)
* *personal or lab website
* In all cases, Email / direct contact was the most frequently used channel: 
* *87% (146) of those who shared did that way
* *82% (82) of those whose data was reused shared it with the reusers that way.
* *57% (86) of those who reused got data that way
* Repository
* *54% (90)
* 8 (5%) wrote in that they extracted data from a published paper.


###Credit
* we wanted to asses what researchers thought was appropriate credit for the creators of a dataset
* the most common response was formal citation in the reference list at 83% (126)
* acknowledgement also ranked highly at 62% (93)
* the most common actual practice <CITATION>, informal reference in the body of the paper, ranked lowest at 16% (24)
	
* we provided an 'Other' option with a free-text field.  
* Overwhelmingly, some variant on "it depends"
* many specified one of two factors:
* *the role of the data in the papers
* **"depends on the amount of data shared and the importance to the study-if just some support in discussion, acknowledgement; vs. half or more of main study, authorship."
* **"authorship if data is primary source of analysis, otherwise acknowledgement"
* **"depends how central the data is to the resulting paper"
* *the publication status of the dataset
* **"Depends on whether the data is already published"
* **"Data should be archived at publication of its first use. After that it can be acknowledged or cited if used in the future."

* because there are substantial differences in culture between disciplines \cite{harley_assessing_2010, swan_share_2008} we examined them separately
* biology and the physical sciences had a relatively high expectation of authorship at 53% (48) and 50% (9) respectively
* surprisingly, discounting Mathematics (b/c of low n) and Other (b/c meaningless), a chi2 test did not find significant differences between disciplines
* *chi2=15.6, p=0.9

* we also asked the subset who had published on someone else's data what they actually did
* results agreed rougly with the responses to the hypothetical
* formal citation rated highest with 63% (81)
* no one admitted to informal citation in the text



###Data vultures
* In credit figure
* 'data vultures' / being taken advantage of is a concern in data publication 
* we wanted to know how much of a problem it currently is
* we asked respondents whose data had been reused in a publication by someone else "how did you feel about the amount of credit you were given?"

* 86 answered the question
* a majority 63% (54) thought they got the right amount of credit
* 78% (67)thought they got sufficient or too much credit
* 22% (19) were unhappy, 2% (2) got 'very insufficient' credit

* belief that formal citation is appropriate credit was associated with more satisfaction 
* *32% (6) insufficient/very insufficient and 5% (1) excessive/very excessive vs.
* *19% (13) insufficient/very insufficient and %18 (12) excessive
* *not significant, chi2 = 2.56, p = 0.28

* belief that authorship is appropriate credit was associated with more dissatisfaction
* *16% (9) insufficient/very insufficient and 15% (8) excessive/very excessive vs.
* *32% (10) insufficient/very insufficient and 16% (5) excessive/very excessive
* *not significant, chi2 = 3.26, p = 0.20



###Definitions
* We decomposed current models of data publication and asked what features researchers would expect a "published" vs a "shared" dataset.
* 

####Data Publication
* in definitions figure
* two most common expectations relate to access & preservation
* *availability at 166 / 68%
* *repository at 133 / 54%
* traditional paper (105 / 43%) more expected than data paper (55 / 22%)
* thorough metadata (97 / 39%) outranks formal metadata (62 / 25%)
* most researchers *don't* expect publication to imply peer review (only 70 / 29%)

####Peer review
* in definitions figure
* Evaluation of metadata ranks highly (196 / 80%)
* *researchers recognize it's importance
* *more respondents expected metadata evaluation from peer review than good metadata from publication (what's that mean?)
* a majority (150 / 62%) expect an evaluation of plausibility that requires domain expertise
* collection and processing methods (220 / 90%) presumably also demand some domain expertise
* low expectation that impact will be considered (53 / 22%)
* An expectation that data publication includes peer review correlated postively with thorough metadata (OR= 5.67, p= 0.00029, alpha= 0.008), plausibility considered (OR= 2.29, p= 0.0087, alpha= 0.008), and novelty/impact considered (OR= 2.42, p= 0.0092, alpha= 0.008), although only the first is statistically significant after accountin for MHT.

####Interrelations
* we wanted to know whether there was some order to the definitions provided, so we tested against the null hypothesis that each component was chosen (or not) independently of every other
* We used ch2 tests within each definition question; the null hypothesis was rejected for both data publication (chi2 = 290.0, p= 6.9e-58) and peer review (chi2 = 343.0, p= 5.7e-72)
* To identify specific correlations, we performed pairwise Fisher Exact tests on every pair of features
* None of the negative associations were close to statistically significant

* Data publication
* *We found a dense set of statiscitally significant (alpha = 0.05, adjusted for multiple hypothesies) associations among features related to access/preservation
* **Deposited in a repository was linked to openly available (or=9.55, p= 7.6e-14), unique ID (or= 3.33, p= 1.33e-5), formal (or= 4.49, p= 3.94e-6) and rich (or= 3.83, p=1.14e-6) metadata.
* **Formal and rich metadata were tightly linked (or= 12.15, p=2.07e-14).
* **Formal metadata is tightly linked to Unique ID (or= 7.92, p=5.05e-11)
* **Peer-review is associated with data papers (or= 3.00, p=0.0011)
* **Data papers are also associated with rich (or=3.64, p=4.22e-5) and formal (or= 4.30, p= 1.32e-05)
* **Traditional paper floats on its own (closest is or= 1.93, p=0.023)

* Peer review
* **Novelty was only associated with Plausibility (or= 5.50, p=1.13e-5)
* **Plausibility, in turn is also associated with approriate methods (or= 4.51, p= 0.0014) and sufficient metadata (or= 4.90, p= 2.51e-6)


###Values

####Data trust
* in values figure
* part of the rationale for data publication is to make reuse of data easier
* we wanted to know what features of a data publication would be useful to researchers in deciding whether to trust a dataset

* easily the biggest factor was peer review
* *175 / 72% said complete or high confidence
* *only 4 / 2% said little or no confidence

* traditional paper was second with 137 / 56% complete or high confidence
* reuse in 3rd, with 106 / 43% 
* finally, data paper was the least convincing with 89 / 37%

* all of the features inspired at least some confidence in a large majority of researchers
* *the largest fraction of little/no confidence was reuse, at 25 / 10%

* no strong relationship between peer review definition and peer review confidence


####Data value/impact
* in values figure
* we wanted to know what dataset metrics researchers would value

* citations, not surprisingly, was the winner
* *119 / 49% extremely or highly useful

* downloads did surprisingly well relative to citations
* *77 / 32% extremely or highly useful
* if somewhat useful is added in, downloads gets even closer to citations 179 (73%) vs. 201 (82%)

* There was a substantial drop for search rank and altmetrics
* *102 (42%) find search rank at least somewhat useful
* *91 (37%) find altmetrics at least somewhat useful 
* * furthermore, a majority of researchers find each to be slightly or not at all useful

####Researcher evaluation
* in values figure
* part of the rationale for data publication is to incentivize data sharing by fitting data into the academic reward system
* to that end, we wanted to know what features of a data publication would increase its value as an item on a CV
* we considered the imprimatur of peer review and the inclusion of a data paper 

* to establish a baseline, we asked about traditional papers
* *not surprisingly, they are highly valued
* *145 (60%) give it a great deal of weight and another 87 (36%) give it significant weight
* we have a lot of biologists, who value peer reviewed publications above all else \cite{harley_assessing_2010}

* data publications were much less highly valued at 23 (10%) a great deal for highest scorer, a peer-reviewed data paper, although 109 (46%) still gave that significant weight.

* peer review appeared to be the more important feature
* *from that 10%, a peer-reviewed dataset only dropped to 5% (12), while an un-peer-reviewed data paper dropped to 0.01% (2)
* *if significant weight is added in, a peer reviewed data paper comes in at 134 (56%), a peer reviewed dataset at 85 (36%), and an un-reviewed data paper at 31 (12%)
* not surprisingly, a significant number of respondents 65 (27%) gave un-peer reviewed data no weight at all.

* we were particularly interested in the 26% (59) respondents who had served on a tenure and promotions committee.
* their responses were similar to those who had not served on such a committee
* we compared the set of values assigned to each item between those who had and had not served on a t&p committee by chi-square
* values ranged from 
* *chi2= 7.427 (p= 0.115) for peer-reviewed datasets (valued slightly less by experienced respondents) to 
* *chi2= 2.097 (p= 0.351) for traditional papers (valued slightly more by experienced respondents)
* for 5 items, alpha=0.01, none can be considered statistically significant


##Discussion
####Outline
* Demographics

* Data sharing

* Data publication
* *Data papers
* *Data publication definition

* Data Validation
* *trustworthy features
* *Peer Review
* **definition

* Data Credit
* *Data Citation
* *Data Vultures
* *Valuable features

* Data publication redux


###Demographics

Geography
* Although our survey was international, the majority of responses came from the US
* This is similar to Tenopir is 73% N. America, we are 84%.
* Likely to be a difference with the big UK reports
* we're heavily in the US, a different envrionment than the UK.
* some of the more thorough surveys have been done in the UK, where a single framework for assessment, the Research Assement Exercise (REA), now the Research Excellence Framework (REF) dominates utterly\cite{swan_share_2008, bobrow_establishing_2014}.
* * at least 75% in Bobrow felt REF doesn't recognize data to some or great extent relative to publications, but should.

Institution
* We got mainly responses from research-oriented academic institutions (76\%, 191)
* Tenopir: 81\% vs. our 85\% academic (research+teaching)
* Ceci's initial survey was all academics
* Scaramozzino was at Cal Poly, teaching-oriented university

Role
* Our survey covered active researchers with good coverage of all of the major roles in acadmenic research.
* Tenopir has 47% professors, 13.5% grad students, we have 41% principle investigator/lab heads and 16% grad students.
* most others restricted to PIs
* Bobrow: 69\% PIs 

Discipline
* Our Largest response was from biologists: (37%), followed by Archaeologists (13%), Social scientists (13%), and Environmental scientists (11%)
* Tenopir: most heavily environmental sciences & ecology (36%), followed by social science (16%) and biology (14%)
* *we're heavy in biology (37%), but with a significant representation from Earth and environmental science (16%) and social science (13%).
* Scaramozzino: 18% biology, highest in Physics and Mathematics
* UK surveys:
* *Swann: 8 research areas: astronomy, chemical crystallography, classics, climate science, genomics, and social and public health sciences – and two interdisciplinary areas – systems biology and the UK’s rural economy and land use programme
* *Bobrow: social science (31.4\%), Epidemiology, (26.8\%), Genetics/genomics (20\%)

Bias
* undoubtedly a biased sample: researchers interested in this stuff are more likely to fill it out
* however, the high number (84%) of respondents who did not name a data journal and the high number unware of federal policies (69/35% never heard of NSF DMP, 122/62% never heard of OSTP) suggests that we're not too highly enriched for people who care about this stuff.
* *compared to EAGDA report, where 60% of respondents were aware of data journals
* also, haphazard distribution
* lowish (n) 
* obviously, web survey gives us a higher n than interviews would


###Data sharing

Importance
* widely held 

Differences between disciplines
* Others have found significant differences in data sharing / scholarly communication culture between disciplines
* Tenopir (2011) \cite{tenopir_data_2011} asked what conditions/credit would be fair in exchange for use of data (you using others and others using yours)
* found statistically significant differences between disciplines for every condition
* "Each discipline has its own norms for making data public"\cite{swan_share_2008}
* we didn't, which might be a function of the lower n or different test (we lumped everything together, they considered each column separately)
* comparison between Tenopir results and ours; authorship is the only truly parallel condition
>Is each of the following conditions a fair exchange for the use of data? 
>Co-authorship on publications resulting from use of the data
> >61% (105) biolgists
> >53% (104) social scientists
> >52% (59) computer science/engineering
> >55% (84) physical sciences
>we asked: How should a researcher who shares data be credited?
>authorship on the paper
> >53% (48) biolgists
> >31% (10) social science
> >27% (3) computer science
> >50% (9) physical science
* pretty good agreement among biologists, physical science.  our #s are lower for social & computer science
* *could be change over time or question, but probably it's just the smaller n
* *again, Tenopir found field specific differences, we didn't really

Channels
* sharing channels
* likely to be distorted by, e.g. the fact that a researcher always knows when someone gets their data though email, might not know about it if it's downloaded from a repository.
* *fitting with this idea, direct contact has a bigger lead in the two questions about other researchers using your data ()


### Data publication
####Definition
* Still clearly a lot of confusion: the most popular feature was only named by about 2/3 of respondents (68\%) 
* *"Terms are confusing"
* Two, arguably three independent conceptions
* *Top two are tighly correlated: Openly available, Depostied in a repository
* *basis of a traditional paper, which was still fairly popular
* *not even a negative relationship
* *Unique ID, both metadatas all correlate with repository
* peer review and data paper are substanially minority expectations

####Data papers
* the majority of respondents (84%) did not name any data journals
* data papers were the least named component of a published dataset.
* data papers confer less trust than any other feature we looked at, but very close to reuse; on a scale of 1-5, data paper is 3.28 (SEM=0.05) and reuse is 3.35 (SEM=0.05); 44\% rank reuse as high or complete confidence, 36\% rank data papers with high or complete confidence
* data papers are valued much less than traditional papers (3.54 SEM=0.05 vs. 4.55 SEM=0.04) and 10% 'A great deal' vs 60%.
* data papers do clearly add value to a dataset on a CV, but peer-reviewed datasets without papers do quite well
* our results don't really resolve the question of whether data papers are a good use of one's time
* the idea of data papers is to fit into the existing scholarly reward system; they don't seem to fit all that much better than datasets
* the hope/presumption/trend is that data is getting more recognition->  the gap might shrink.  on the other hand, no one thinks data papers *should* be considered as valuable as research papers, so perhaps this is all fine
* "I've never heard of this, but it sounds fantastic."



###Validation
* Data pub definition: "There is, for some, also an implication that the information has been through a quality control process." \cite{swan_share_2008}
* Validation practices vary greatly between implementations.
* traditional model, traditional paper is still highly trusted 

####Peer review
* both the biggest factor in evaluating the trustworthiness of a dataset and the value on a CV
* peer review is valued, but not expected (71% don't think publish implies peer review)

* expectations for peer review seem to be more consistent than publication: appropriate methods was named by fully 90\% of respondents
* *half of the items were named by more respondents than the highest item in data pub definition.

* a majority of researchers expect peer review to involve assessments that require domain expertise
* "data is plausible" is associated with 3 other features that depend on domain knowledge (methods, metadata to replicate, novelty)
* low expectation that impact would be considered fits with the practices of current DP efforts
* data publication initiatives should probably incorporate some form of peer review if they can

* "I have never heard this term applied to a dataset and I don't know what it means"
* "I wouldn't expect metadata to be standardized, but I would like it if it were!"
* we have a lot of biologists, who value peer reviewed publications above all else \cite{harley_assessing_2010}

####Use as review
* Successful reuse was rated relatively low, to our surprise
* personal experience
* “data use in its own right provides a form of review”\cite{parsons_data_2010}
* basis of a traditional paper did better: not reuse, but use.
* one might conclude from that that efforts to catalog citations or collect feedback from users may not be taken seriously
* however, citations were the strongest factor in assessing value
* *could be difference in trust and value, could be difference in reuse and citation
* probably best not to rush ahead with this

###Credit

###Data vultures 
* although the majority of respondents were satisfied, the fraction who were not (22%) is too large to ignore
* In kim (2012) study, 32% (8/25) researchers "Worried about data theft; cannot trust others" /cite{kim_institutional_2012}
* no judgement implied by us about whether researcher expectations are reasonable or not
* could potentially address by some combination of encouraging people to credit more and expect less
* maybe disciplines will work it out for themselves



####Citation
* there is consensus in the scholarly communication community that the right thing to do is to cite formally in the reference list \cite{force11_data_citation_synthesis_group_joint_2014}
* however, past studies have found that this is rarely actually done.\cite{sieber_not_1995, mooney_citing_2011, mooney_anatomy_2012}
* In a 1995 survey of 198 papers that used published datasets, Sieber and Trumbo found that only 18.7% (37) had a citation with at least the title in the reference list \cite{sieber_not_1995}.  Seven years later, Mooney and Newton conducted a followup survey of 65 papers; only 16.9% (11) cited in the reference list.
* In the sieber survey, 30.8% (61) of papers included the title of the dataset in the text of the paper; for Mooney it was 69.2% (45).

* here, most respondents agreed that formal citation is the way to go.  This was the most common response to both how a datset creator should be credited 83% (126) and how they actually did credit the creator of a dataset that they used 63% (81) 
* furthermore, none of them would admit to informally citing data in the body of the paper; could be confusion over terminology, could be differences in the group, could be lack of self-awareness.
* there was fairly good agreement between the hypothetical and the reported actual behavior.
* *still, statistically significant difference chi2= 15.8, p=0.0013
* *obviously, self-reported behavior comes with caveats
* *e.g. very likely that a bio researcher using a GenBank sequence won't view that as data reuse

* suggests that what we need to do is enable data citation, not make the case in the first place


####Metrics / valuable features
* citations are, not surprisingly, the most valued
* however, downloads do well and are much easier for a data publisher to implement
* *if you're publishing data, you should provide download statistics
* altmetrics still have a ways to go
* *"Why would any of the things listed matter?"
* *while few give them much weight, a majority do give them at least a little
* "Eighty-three per cent of respondents already tracked the use of their datasets, primarily through details of publications generated using the data (71%), the number of researchers accessing the data (51%) and the number of data downloads (43%)."\cite{bobrow_establishing_2014}
 

###Data publication redux
* What is a data publication?
* available: open, secondarily in a repository
* citable: didn't ask specificially, but formal citation was the most appropriate way to cite a paper; repository and ID are citation-enablers
* validated: 


* What should data publishers do?


* Although there is a lot of uncertainty still about what data publication means,
1. gotta be in a repository, provide access
4. must be citable, tracking ciations those is valuable.  But, downloads are good as a proxy.
2. doesn't have to be peer reviewed, but peer review is really good to have both for reusers and to give weight to the work involved.
3. Data papers add weight and trust, but less than peer review.
(maybe invert these to end on access)



#Tables & Figures

##Demographics
* Discipline, role, highest degree, and institution-type
* analyzed responses from 249 researchers
* Most common: Biologist, PI, PhD, research-focused academic

##Policy Awareness
* Respondents were asked to rate their familiarity with three US government policies related to data sharing
* *scale from "never heard of it" to "know all the details"
* Analysis restricted to US respondents
* (A) OSTP Open Data Initiative (n=197)
* (B) NSF DMP requirements (n=197)
* (C) NIH data sharing policy: only relevant to biologists (n=71)

##Sharing Experience
* restricted to respondents who answered "yes" "to "have you shared data?"
* (A) - (C) are concerned with channels for sharing data
* *Email / direct contact
* *database or repository
* *journal website (as supplemental material)
* *personal or lab website
* (A) Through what channels have you shared data? (n=122)
* (B) If your data has been reused by someone else, how did they get it?
* (C) If you have reused someone else's data, how did you get it? (n=150)
* (D) looks at accompanying documentation (n=121)
* *A traditional research paper based on the data (with analysis and conclusions)
* *A data paper describing the data (without analysis or conclusions)
* *Informal text describing the data
* *Formal metadata describing the data (e.g. as XML)
* *Computer code used to process or generate the data
* *Shared with no additional documentation

##Credit
* (A, B) mechanisms for credit
* *Authorship on paper
* *Acknowledgement in the paper
* *Data cited in the reference list
* *Data cited informally in the text of the paper
* (A) theoretical: how data sharer should be credited (n=151)
* (B) concrete: how the respondent credited others (n=129)
* (C) how satisfied they were with the credit they got (n=86)
* *scale of 1-5 from very insufficient to very excessive
* *mean = 2.9, SE = 0.8

##Definitions
* data publication (vs. sharing) and peer review

##Values
* data trust, data value, data publication value


#Scrap
* at 22% peer review is low on the list (6/8) of expected publication features, but it's highly valued

* people who haven't reviewed a journal article value journal articles less and everything else more than people who have
* *nothing significant at p = 0.01

* Scaramozzino (2012) \cite{scaramozzino_study_2012} survey of 131 tenure-track faculty at a teaching-oriented university (Cal Poly)
>Over 65 percent of respondents believe it is important that they openly share their data and that their colleagues do the same. Of those who believe it is important, fewer than half (48%) report always or frequently sharing data with those outside their research group.

*Kim (2012) \cite{kim_insitutional_2012} interviewed 25 researchers about data sharing.
* *15 of them mentioned credit/reputation as a benefit.
* *methods mentioned: on request/email, personal website, external repository, supplemental material

* Differences in actual and reported behavior: Ceci (1988) surveyed 57 "nonacademic scientists working in both the private and the public sectors at places like the Centers for Disease Control, the National Insitutes of Health, and various research hospitals" and found that 87% reported routinely honoring requests for data, but "59% claimed that their colleagues were not prone to sharing their data, even when they were obtained with the benefit of federal funds." \cite{ceci_scientists_1988}

###Data sharing surveys
<!--- consider what's relevant to data pub/our survey: credit, value, etc. -->

* Lots of surveys of researcher practices and perceptions around data sharing \cite{ceci_scientists_1988, swan_share_2008, harley_assessing_2010, tenopir_data_2011, kim_institutional_2012, scaramozzino_study_2012}
* papers:
* *Ceci (1988) 790 academic 'researchers from a variety of fields' in spring 1985, followed by 57 nonacademic.  "Scientist's Attitudes toward Data Sharing" \cite{ceci_scientists_1988}
* *Tenopir surveyed 1329 scientists on their practices and perceptions of data sharing in 2011.\cite{tenopir_data_2011}
* *Kim (2012) \cite{kim_insitutional_2012} interviewed 25 researchers about data sharing.
* *Scaramozzino (2012) \cite{scaramozzino_study_2012} survey of 131 tenure-track faculty at a teaching-oriented university (Cal Poly)
* reports
* *RIN report based on >100 "detailed interviews with researchers across 8 subject areas" in UK addresses data publciation explicitly \cite{swan_share_2008}
* *Assessing the Future Landscape of Scholarly Communication: 160 interviews over 7 fields, covers lots of topics, but data sharing is in there \cite{harley_assessing_2010}
* *"The Scientific Method in Practice: Reproducibility in the Computational Sciences"\cite{stodden_scientific_2010}

* Expert Advisory Group on Data Access (EAGDA) Report includes survey of 35 researchers over 3 fields in UK in 2013 \cite{bobrow_establishing_2014, }

Published data can accumulate citations credit to compensate researchers for the time spent preparing their data. 
The risk of missing out on publications is at least slightly offset if the researcher's contribution will be recognized.
Data publications will be entered into the scholarly record for preservation.


* conclusions:
* *time is the most highly cited cost \cite{tenopir_data_2011, kim_institutional_2012} to data sharing.

Commenting on the variable meanings of data publication to researchers, the RIN report of 2008 remarked, ``[t]here is, for some, also an implication that the information has been through a quality control process.''\cite{swan_share_2008}